{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7bc98a6-96a0-4899-9b1b-2686b17f1e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, VectorSlicer, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0722f2bf-4663-4034-8ad2-28be805d8c33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#file_path = '/FileStore/tables/'\n",
    "file_path = './'\n",
    "\n",
    "train_df = spark.read.csv(file_path+'train_features.csv', header=True, inferSchema=True)\n",
    "target_df = spark.read.csv(file_path+'train_targets_scored.csv', header=True, inferSchema=True)\n",
    "train_drug_df = spark.read.csv(file_path+'train_drug.csv', header=True, inferSchema=True)\n",
    "target_nonscored_df = spark.read.csv(file_path+'train_targets_nonscored.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(file_path+'test_features.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 'sig_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdf91bbe-590f-45f7-8fcd-29af4fd9d553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, cp_type: string, cp_time: int, cp_dose: string, g-0: double, g-1: double, g-2: double, g-3: double, g-4: double, g-5: double, g-6: double, g-7: double, g-8: double, g-9: double, g-10: double, g-11: double, g-12: double, g-13: double, g-14: double, g-15: double, g-16: double, g-17: double, g-18: double, g-19: double, g-20: double, g-21: double, g-22: double, g-23: double, g-24: double, g-25: double, g-26: double, g-27: double, g-28: double, g-29: double, g-30: double, g-31: double, g-32: double, g-33: double, g-34: double, g-35: double, g-36: double, g-37: double, g-38: double, g-39: double, g-40: double, g-41: double, g-42: double, g-43: double, g-44: double, g-45: double, g-46: double, g-47: double, g-48: double, g-49: double, g-50: double, g-51: double, g-52: double, g-53: double, g-54: double, g-55: double, g-56: double, g-57: double, g-58: double, g-59: double, g-60: double, g-61: double, g-62: double, g-63: double, g-64: double, g-65: double, g-66: double, g-67: double, g-68: double, g-69: double, g-70: double, g-71: double, g-72: double, g-73: double, g-74: double, g-75: double, g-76: double, g-77: double, g-78: double, g-79: double, g-80: double, g-81: double, g-82: double, g-83: double, g-84: double, g-85: double, g-86: double, g-87: double, g-88: double, g-89: double, g-90: double, g-91: double, g-92: double, g-93: double, g-94: double, g-95: double, g-96: double, g-97: double, g-98: double, g-99: double, g-100: double, g-101: double, g-102: double, g-103: double, g-104: double, g-105: double, g-106: double, g-107: double, g-108: double, g-109: double, g-110: double, g-111: double, g-112: double, g-113: double, g-114: double, g-115: double, g-116: double, g-117: double, g-118: double, g-119: double, g-120: double, g-121: double, g-122: double, g-123: double, g-124: double, g-125: double, g-126: double, g-127: double, g-128: double, g-129: double, g-130: double, g-131: double, g-132: double, g-133: double, g-134: double, g-135: double, g-136: double, g-137: double, g-138: double, g-139: double, g-140: double, g-141: double, g-142: double, g-143: double, g-144: double, g-145: double, g-146: double, g-147: double, g-148: double, g-149: double, g-150: double, g-151: double, g-152: double, g-153: double, g-154: double, g-155: double, g-156: double, g-157: double, g-158: double, g-159: double, g-160: double, g-161: double, g-162: double, g-163: double, g-164: double, g-165: double, g-166: double, g-167: double, g-168: double, g-169: double, g-170: double, g-171: double, g-172: double, g-173: double, g-174: double, g-175: double, g-176: double, g-177: double, g-178: double, g-179: double, g-180: double, g-181: double, g-182: double, g-183: double, g-184: double, g-185: double, g-186: double, g-187: double, g-188: double, g-189: double, g-190: double, g-191: double, g-192: double, g-193: double, g-194: double, g-195: double, g-196: double, g-197: double, g-198: double, g-199: double, g-200: double, g-201: double, g-202: double, g-203: double, g-204: double, g-205: double, g-206: double, g-207: double, g-208: double, g-209: double, g-210: double, g-211: double, g-212: double, g-213: double, g-214: double, g-215: double, g-216: double, g-217: double, g-218: double, g-219: double, g-220: double, g-221: double, g-222: double, g-223: double, g-224: double, g-225: double, g-226: double, g-227: double, g-228: double, g-229: double, g-230: double, g-231: double, g-232: double, g-233: double, g-234: double, g-235: double, g-236: double, g-237: double, g-238: double, g-239: double, g-240: double, g-241: double, g-242: double, g-243: double, g-244: double, g-245: double, g-246: double, g-247: double, g-248: double, g-249: double, g-250: double, g-251: double, g-252: double, g-253: double, g-254: double, g-255: double, g-256: double, g-257: double, g-258: double, g-259: double, g-260: double, g-261: double, g-262: double, g-263: double, g-264: double, g-265: double, g-266: double, g-267: double, g-268: double, g-269: double, g-270: double, g-271: double, g-272: double, g-273: double, g-274: double, g-275: double, g-276: double, g-277: double, g-278: double, g-279: double, g-280: double, g-281: double, g-282: double, g-283: double, g-284: double, g-285: double, g-286: double, g-287: double, g-288: double, g-289: double, g-290: double, g-291: double, g-292: double, g-293: double, g-294: double, g-295: double, g-296: double, g-297: double, g-298: double, g-299: double, g-300: double, g-301: double, g-302: double, g-303: double, g-304: double, g-305: double, g-306: double, g-307: double, g-308: double, g-309: double, g-310: double, g-311: double, g-312: double, g-313: double, g-314: double, g-315: double, g-316: double, g-317: double, g-318: double, g-319: double, g-320: double, g-321: double, g-322: double, g-323: double, g-324: double, g-325: double, g-326: double, g-327: double, g-328: double, g-329: double, g-330: double, g-331: double, g-332: double, g-333: double, g-334: double, g-335: double, g-336: double, g-337: double, g-338: double, g-339: double, g-340: double, g-341: double, g-342: double, g-343: double, g-344: double, g-345: double, g-346: double, g-347: double, g-348: double, g-349: double, g-350: double, g-351: double, g-352: double, g-353: double, g-354: double, g-355: double, g-356: double, g-357: double, g-358: double, g-359: double, g-360: double, g-361: double, g-362: double, g-363: double, g-364: double, g-365: double, g-366: double, g-367: double, g-368: double, g-369: double, g-370: double, g-371: double, g-372: double, g-373: double, g-374: double, g-375: double, g-376: double, g-377: double, g-378: double, g-379: double, g-380: double, g-381: double, g-382: double, g-383: double, g-384: double, g-385: double, g-386: double, g-387: double, g-388: double, g-389: double, g-390: double, g-391: double, g-392: double, g-393: double, g-394: double, g-395: double, g-396: double, g-397: double, g-398: double, g-399: double, g-400: double, g-401: double, g-402: double, g-403: double, g-404: double, g-405: double, g-406: double, g-407: double, g-408: double, g-409: double, g-410: double, g-411: double, g-412: double, g-413: double, g-414: double, g-415: double, g-416: double, g-417: double, g-418: double, g-419: double, g-420: double, g-421: double, g-422: double, g-423: double, g-424: double, g-425: double, g-426: double, g-427: double, g-428: double, g-429: double, g-430: double, g-431: double, g-432: double, g-433: double, g-434: double, g-435: double, g-436: double, g-437: double, g-438: double, g-439: double, g-440: double, g-441: double, g-442: double, g-443: double, g-444: double, g-445: double, g-446: double, g-447: double, g-448: double, g-449: double, g-450: double, g-451: double, g-452: double, g-453: double, g-454: double, g-455: double, g-456: double, g-457: double, g-458: double, g-459: double, g-460: double, g-461: double, g-462: double, g-463: double, g-464: double, g-465: double, g-466: double, g-467: double, g-468: double, g-469: double, g-470: double, g-471: double, g-472: double, g-473: double, g-474: double, g-475: double, g-476: double, g-477: double, g-478: double, g-479: double, g-480: double, g-481: double, g-482: double, g-483: double, g-484: double, g-485: double, g-486: double, g-487: double, g-488: double, g-489: double, g-490: double, g-491: double, g-492: double, g-493: double, g-494: double, g-495: double, g-496: double, g-497: double, g-498: double, g-499: double, g-500: double, g-501: double, g-502: double, g-503: double, g-504: double, g-505: double, g-506: double, g-507: double, g-508: double, g-509: double, g-510: double, g-511: double, g-512: double, g-513: double, g-514: double, g-515: double, g-516: double, g-517: double, g-518: double, g-519: double, g-520: double, g-521: double, g-522: double, g-523: double, g-524: double, g-525: double, g-526: double, g-527: double, g-528: double, g-529: double, g-530: double, g-531: double, g-532: double, g-533: double, g-534: double, g-535: double, g-536: double, g-537: double, g-538: double, g-539: double, g-540: double, g-541: double, g-542: double, g-543: double, g-544: double, g-545: double, g-546: double, g-547: double, g-548: double, g-549: double, g-550: double, g-551: double, g-552: double, g-553: double, g-554: double, g-555: double, g-556: double, g-557: double, g-558: double, g-559: double, g-560: double, g-561: double, g-562: double, g-563: double, g-564: double, g-565: double, g-566: double, g-567: double, g-568: double, g-569: double, g-570: double, g-571: double, g-572: double, g-573: double, g-574: double, g-575: double, g-576: double, g-577: double, g-578: double, g-579: double, g-580: double, g-581: double, g-582: double, g-583: double, g-584: double, g-585: double, g-586: double, g-587: double, g-588: double, g-589: double, g-590: double, g-591: double, g-592: double, g-593: double, g-594: double, g-595: double, g-596: double, g-597: double, g-598: double, g-599: double, g-600: double, g-601: double, g-602: double, g-603: double, g-604: double, g-605: double, g-606: double, g-607: double, g-608: double, g-609: double, g-610: double, g-611: double, g-612: double, g-613: double, g-614: double, g-615: double, g-616: double, g-617: double, g-618: double, g-619: double, g-620: double, g-621: double, g-622: double, g-623: double, g-624: double, g-625: double, g-626: double, g-627: double, g-628: double, g-629: double, g-630: double, g-631: double, g-632: double, g-633: double, g-634: double, g-635: double, g-636: double, g-637: double, g-638: double, g-639: double, g-640: double, g-641: double, g-642: double, g-643: double, g-644: double, g-645: double, g-646: double, g-647: double, g-648: double, g-649: double, g-650: double, g-651: double, g-652: double, g-653: double, g-654: double, g-655: double, g-656: double, g-657: double, g-658: double, g-659: double, g-660: double, g-661: double, g-662: double, g-663: double, g-664: double, g-665: double, g-666: double, g-667: double, g-668: double, g-669: double, g-670: double, g-671: double, g-672: double, g-673: double, g-674: double, g-675: double, g-676: double, g-677: double, g-678: double, g-679: double, g-680: double, g-681: double, g-682: double, g-683: double, g-684: double, g-685: double, g-686: double, g-687: double, g-688: double, g-689: double, g-690: double, g-691: double, g-692: double, g-693: double, g-694: double, g-695: double, g-696: double, g-697: double, g-698: double, g-699: double, g-700: double, g-701: double, g-702: double, g-703: double, g-704: double, g-705: double, g-706: double, g-707: double, g-708: double, g-709: double, g-710: double, g-711: double, g-712: double, g-713: double, g-714: double, g-715: double, g-716: double, g-717: double, g-718: double, g-719: double, g-720: double, g-721: double, g-722: double, g-723: double, g-724: double, g-725: double, g-726: double, g-727: double, g-728: double, g-729: double, g-730: double, g-731: double, g-732: double, g-733: double, g-734: double, g-735: double, g-736: double, g-737: double, g-738: double, g-739: double, g-740: double, g-741: double, g-742: double, g-743: double, g-744: double, g-745: double, g-746: double, g-747: double, g-748: double, g-749: double, g-750: double, g-751: double, g-752: double, g-753: double, g-754: double, g-755: double, g-756: double, g-757: double, g-758: double, g-759: double, g-760: double, g-761: double, g-762: double, g-763: double, g-764: double, g-765: double, g-766: double, g-767: double, g-768: double, g-769: double, g-770: double, g-771: double, c-0: double, c-1: double, c-2: double, c-3: double, c-4: double, c-5: double, c-6: double, c-7: double, c-8: double, c-9: double, c-10: double, c-11: double, c-12: double, c-13: double, c-14: double, c-15: double, c-16: double, c-17: double, c-18: double, c-19: double, c-20: double, c-21: double, c-22: double, c-23: double, c-24: double, c-25: double, c-26: double, c-27: double, c-28: double, c-29: double, c-30: double, c-31: double, c-32: double, c-33: double, c-34: double, c-35: double, c-36: double, c-37: double, c-38: double, c-39: double, c-40: double, c-41: double, c-42: double, c-43: double, c-44: double, c-45: double, c-46: double, c-47: double, c-48: double, c-49: double, c-50: double, c-51: double, c-52: double, c-53: double, c-54: double, c-55: double, c-56: double, c-57: double, c-58: double, c-59: double, c-60: double, c-61: double, c-62: double, c-63: double, c-64: double, c-65: double, c-66: double, c-67: double, c-68: double, c-69: double, c-70: double, c-71: double, c-72: double, c-73: double, c-74: double, c-75: double, c-76: double, c-77: double, c-78: double, c-79: double, c-80: double, c-81: double, c-82: double, c-83: double, c-84: double, c-85: double, c-86: double, c-87: double, c-88: double, c-89: double, c-90: double, c-91: double, c-92: double, c-93: double, c-94: double, c-95: double, c-96: double, c-97: double, c-98: double, c-99: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "target_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02385597-27bc-418a-8725-873b1b3f2022",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode_cat_features(df, cat_features):\n",
    "\n",
    "  indexed_cols = [''.join([col_name, '_indexed']) for col_name in cat_features]\n",
    "  encoded_cols = [''.join([col_name, '_encoded']) for col_name in cat_features]\n",
    "  string_indexers = [StringIndexer(inputCol=cat_features[i], outputCol=indexed_cols[i]) for i in range(len(cat_features))]\n",
    "    \n",
    "  encoder = OneHotEncoder(inputCols=indexed_cols, outputCols=encoded_cols)\n",
    "  \n",
    "  pipline = Pipeline(stages=string_indexers + [encoder])\n",
    "  \n",
    "  encoded_df = pipline.fit(df).transform(df)\n",
    "  encoded_df = encoded_df.drop(*indexed_cols + cat_features)\n",
    "\n",
    "  return encoded_df\n",
    "\n",
    "def normalize_features(df, cols, normalizer, output_cols, if_drop=True):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  normalizer_lst = []\n",
    "  vectorized_cols = []\n",
    "  vector_assembers = []\n",
    "  \n",
    "  if isinstance(cols, list):\n",
    "    cols = {'cols': cols}\n",
    "  \n",
    "  if isinstance(output_cols, str):\n",
    "    output_cols = {'cols': output_cols}\n",
    "  \n",
    "  for k, v in cols.items():\n",
    "    \n",
    "    temp_normalizer = normalizer.copy()\n",
    "    vectorized_col = ''.join([output_cols[k], '_v'])\n",
    "    vector_assember = VectorAssembler(inputCols=v, outputCol=vectorized_col)\n",
    "    \n",
    "    temp_normalizer.setInputCol(vectorized_col)\n",
    "    temp_normalizer.setOutputCol(output_cols[k])\n",
    "    \n",
    "    normalizer_lst.append(temp_normalizer)\n",
    "    vectorized_cols.append(vectorized_col)\n",
    "    vector_assembers.append(vector_assember)\n",
    "  \n",
    "  pipline = Pipeline(stages=vector_assembers + normalizer_lst)\n",
    "  normalized_df = pipline.fit(df).transform(df).drop(*vectorized_cols)\n",
    "  \n",
    "  if if_drop:\n",
    "    \n",
    "    for k, v in cols.items():\n",
    "      \n",
    "      normalized_df = normalized_df.drop(v)\n",
    "  \n",
    "  return normalized_df\n",
    "\n",
    "def add_pca_features(df, g_cols, c_cols, k=40):\n",
    "  \n",
    "  ## normalize g-col and c-col\n",
    "  std_scaler = StandardScaler(withMean=True)\n",
    "  \n",
    "  input_cols = {\n",
    "    'g_cols': g_cols, \n",
    "    'c_cols': c_cols}\n",
    "  \n",
    "  output_cols = {\n",
    "    'g_cols': 'g_normalized', \n",
    "    'c_cols': 'c_normalized'}\n",
    "  \n",
    "  normalized_df = normalize_features(df, input_cols, std_scaler, output_cols, if_drop=False)\n",
    "  \n",
    "  ## perform PCA on g-cols and c-cols\n",
    "  g_col_pca = PCA(k=k, inputCol='g_normalized', outputCol='g_col_pca')\n",
    "  c_col_pca = PCA(k=k, inputCol='c_normalized', outputCol='c_col_pca')\n",
    "  \n",
    "  pipeline = Pipeline(stages=[g_col_pca, c_col_pca])\n",
    "  pca_df = pipeline.fit(normalized_df).transform(normalized_df)\n",
    "  \n",
    "  return pca_df\n",
    "  \n",
    "def add_stats_features(df, g_cols, c_cols):\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_sum(*lst):\n",
    "\n",
    "    return sum(lst)\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_mean(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst)\n",
    "\n",
    "    return s / n\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_var(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst) / n\n",
    "    total = 0\n",
    "\n",
    "    for x in lst:\n",
    "\n",
    "      total += (x - s)**2 \n",
    "\n",
    "    return total / n\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_min(*lst):\n",
    "    \n",
    "    return min(lst)\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_max(*lst):\n",
    "    \n",
    "    return max(lst)\n",
    "  \n",
    "  stats_dict = {\n",
    "    'min_stats': cols_min,\n",
    "    'max_stats': cols_max,\n",
    "    'var_stats': cols_var,\n",
    "    'mean_stats': cols_mean,\n",
    "    'sum_stats': cols_sum\n",
    "  }\n",
    "  \n",
    "  for name, func in stats_dict.items():\n",
    "    \n",
    "    df = df.withColumn(''.join(['g_cols_', name]), func(*[col(g_col) for g_col in g_cols]))\n",
    "    df = df.withColumn(''.join(['c_cols_', name]), func(*[col(c_col) for c_col in c_cols]))\n",
    "  \n",
    "  return df\n",
    "\n",
    "def add_kmeans_features(df, g_cols, c_cols, k=2, num_iter=10):\n",
    "  \n",
    "  kmeans_g = KMeans(k=k, featuresCol=g_cols, predictionCol='g_col_k_mean', seed=16)\n",
    "  kmeans_c = KMeans(k=k, featuresCol=c_cols, predictionCol='c_col_k_mean', seed=16)\n",
    "  \n",
    "  kmeans_df = kmeans_g.fit(df).transform(df)\n",
    "  kmeans_df = kmeans_c.fit(kmeans_df).transform(kmeans_df)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def feature_engineering(df, num_cluster=2, num_comp=40, num_iter=10):\n",
    "  \n",
    "  ## get g-col and c-col\n",
    "  g_cols = list(filter(lambda v: re.match('g-.+', v), df.columns))\n",
    "  c_cols = list(filter(lambda v: re.match('c-.+', v), df.columns))\n",
    "  \n",
    "  ## PCA\n",
    "  pca_df = add_pca_features(df, g_cols, c_cols, num_comp)\n",
    "\n",
    "  ## stats features on g and c cols\n",
    "  stats_df = add_stats_features(pca_df, g_cols, c_cols)\n",
    "  \n",
    "  ## add k-means features\n",
    "  kmeans_df = add_kmeans_features(stats_df, g_cols='g_normalized', c_cols='c_normalized', k=num_cluster, num_iter=num_iter)\n",
    "  \n",
    "  return kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7a98442-7ce1-4a03-afeb-28cf20d9c28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## add indicator column to both train and test so we can combine them later\n",
    "train_df = train_df.withColumn('is_test', lit(0))\n",
    "test_df = test_df.withColumn('is_test', lit(1))\n",
    "\n",
    "## Combine train and test df\n",
    "full_df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00beab73-8c18-4835-b783-bede27b844ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## encode features\n",
    "target_cols = ['cp_type', 'cp_dose']\n",
    "encoded_df = encode_cat_features(full_df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55343a7f-035e-4f85-903f-5db6e44f50e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## feature engineering\n",
    "fe_df = feature_engineering(encoded_df, num_comp=20, num_iter=5)\n",
    "\n",
    "## select all the feature columns\n",
    "\n",
    "pca_cols = list(filter(lambda v: re.match('.+_pca', v), fe_df.columns))\n",
    "stats_cols = list(filter(lambda v: re.match('.+_stats', v), fe_df.columns))\n",
    "k_means_cols = list(filter(lambda v: re.match('.+_k_mean', v), fe_df.columns))\n",
    "cat_cols = list(filter(lambda v: re.match('.+_encoded', v), fe_df.columns)) + ['cp_time']\n",
    "\n",
    "## stack them to a single feature vector\n",
    "vector_assember_train = VectorAssembler(inputCols=pca_cols + stats_cols + k_means_cols + cat_cols, outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df)\n",
    "\n",
    "## normalize all the features\n",
    "normalizer = StandardScaler(withMean=True)\n",
    "cols = ['all_features']\n",
    "output_cols = 'features'\n",
    "fe_df = normalize_features(fe_df, cols, normalizer, output_cols, if_drop=False)\n",
    "\n",
    "## split train, test df\n",
    "fe_train = fe_df.filter(fe_df['is_test'] == 0)\n",
    "final_test = fe_df.filter(fe_df['is_test'] == 1).select(['sig_id', 'features'])\n",
    "\n",
    "## stack the training target\n",
    "vector_assember_target = VectorAssembler(inputCols=[c for c in target_df.columns if c not in {'sig_id'}], outputCol='targets')\n",
    "vectorized_target = vector_assember_target.transform(target_df).select(['sig_id', 'targets'])\n",
    "\n",
    "## join training target with training features\n",
    "final_train = fe_train.join(vectorized_target, ['sig_id']).select(['sig_id', 'features', 'targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bdf7e32a-ca3a-4de7-b522-31f1137d676c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, features: vector, 5-alpha_reductase_inhibitor: int, 11-beta-hsd1_inhibitor: int, acat_inhibitor: int, acetylcholine_receptor_agonist: int, acetylcholine_receptor_antagonist: int, acetylcholinesterase_inhibitor: int, adenosine_receptor_agonist: int, adenosine_receptor_antagonist: int, adenylyl_cyclase_activator: int, adrenergic_receptor_agonist: int, adrenergic_receptor_antagonist: int, akt_inhibitor: int, aldehyde_dehydrogenase_inhibitor: int, alk_inhibitor: int, ampk_activator: int, analgesic: int, androgen_receptor_agonist: int, androgen_receptor_antagonist: int, anesthetic_-_local: int, angiogenesis_inhibitor: int, angiotensin_receptor_antagonist: int, anti-inflammatory: int, antiarrhythmic: int, antibiotic: int, anticonvulsant: int, antifungal: int, antihistamine: int, antimalarial: int, antioxidant: int, antiprotozoal: int, antiviral: int, apoptosis_stimulant: int, aromatase_inhibitor: int, atm_kinase_inhibitor: int, atp-sensitive_potassium_channel_antagonist: int, atp_synthase_inhibitor: int, atpase_inhibitor: int, atr_kinase_inhibitor: int, aurora_kinase_inhibitor: int, autotaxin_inhibitor: int, bacterial_30s_ribosomal_subunit_inhibitor: int, bacterial_50s_ribosomal_subunit_inhibitor: int, bacterial_antifolate: int, bacterial_cell_wall_synthesis_inhibitor: int, bacterial_dna_gyrase_inhibitor: int, bacterial_dna_inhibitor: int, bacterial_membrane_integrity_inhibitor: int, bcl_inhibitor: int, bcr-abl_inhibitor: int, benzodiazepine_receptor_agonist: int, beta_amyloid_inhibitor: int, bromodomain_inhibitor: int, btk_inhibitor: int, calcineurin_inhibitor: int, calcium_channel_blocker: int, cannabinoid_receptor_agonist: int, cannabinoid_receptor_antagonist: int, carbonic_anhydrase_inhibitor: int, casein_kinase_inhibitor: int, caspase_activator: int, catechol_o_methyltransferase_inhibitor: int, cc_chemokine_receptor_antagonist: int, cck_receptor_antagonist: int, cdk_inhibitor: int, chelating_agent: int, chk_inhibitor: int, chloride_channel_blocker: int, cholesterol_inhibitor: int, cholinergic_receptor_antagonist: int, coagulation_factor_inhibitor: int, corticosteroid_agonist: int, cyclooxygenase_inhibitor: int, cytochrome_p450_inhibitor: int, dihydrofolate_reductase_inhibitor: int, dipeptidyl_peptidase_inhibitor: int, diuretic: int, dna_alkylating_agent: int, dna_inhibitor: int, dopamine_receptor_agonist: int, dopamine_receptor_antagonist: int, egfr_inhibitor: int, elastase_inhibitor: int, erbb2_inhibitor: int, estrogen_receptor_agonist: int, estrogen_receptor_antagonist: int, faah_inhibitor: int, farnesyltransferase_inhibitor: int, fatty_acid_receptor_agonist: int, fgfr_inhibitor: int, flt3_inhibitor: int, focal_adhesion_kinase_inhibitor: int, free_radical_scavenger: int, fungal_squalene_epoxidase_inhibitor: int, gaba_receptor_agonist: int, gaba_receptor_antagonist: int, gamma_secretase_inhibitor: int, glucocorticoid_receptor_agonist: int, glutamate_inhibitor: int, glutamate_receptor_agonist: int, glutamate_receptor_antagonist: int, gonadotropin_receptor_agonist: int, gsk_inhibitor: int, hcv_inhibitor: int, hdac_inhibitor: int, histamine_receptor_agonist: int, histamine_receptor_antagonist: int, histone_lysine_demethylase_inhibitor: int, histone_lysine_methyltransferase_inhibitor: int, hiv_inhibitor: int, hmgcr_inhibitor: int, hsp_inhibitor: int, igf-1_inhibitor: int, ikk_inhibitor: int, imidazoline_receptor_agonist: int, immunosuppressant: int, insulin_secretagogue: int, insulin_sensitizer: int, integrin_inhibitor: int, jak_inhibitor: int, kit_inhibitor: int, laxative: int, leukotriene_inhibitor: int, leukotriene_receptor_antagonist: int, lipase_inhibitor: int, lipoxygenase_inhibitor: int, lxr_agonist: int, mdm_inhibitor: int, mek_inhibitor: int, membrane_integrity_inhibitor: int, mineralocorticoid_receptor_antagonist: int, monoacylglycerol_lipase_inhibitor: int, monoamine_oxidase_inhibitor: int, monopolar_spindle_1_kinase_inhibitor: int, mtor_inhibitor: int, mucolytic_agent: int, neuropeptide_receptor_antagonist: int, nfkb_inhibitor: int, nicotinic_receptor_agonist: int, nitric_oxide_donor: int, nitric_oxide_production_inhibitor: int, nitric_oxide_synthase_inhibitor: int, norepinephrine_reuptake_inhibitor: int, nrf2_activator: int, opioid_receptor_agonist: int, opioid_receptor_antagonist: int, orexin_receptor_antagonist: int, p38_mapk_inhibitor: int, p-glycoprotein_inhibitor: int, parp_inhibitor: int, pdgfr_inhibitor: int, pdk_inhibitor: int, phosphodiesterase_inhibitor: int, phospholipase_inhibitor: int, pi3k_inhibitor: int, pkc_inhibitor: int, potassium_channel_activator: int, potassium_channel_antagonist: int, ppar_receptor_agonist: int, ppar_receptor_antagonist: int, progesterone_receptor_agonist: int, progesterone_receptor_antagonist: int, prostaglandin_inhibitor: int, prostanoid_receptor_antagonist: int, proteasome_inhibitor: int, protein_kinase_inhibitor: int, protein_phosphatase_inhibitor: int, protein_synthesis_inhibitor: int, protein_tyrosine_kinase_inhibitor: int, radiopaque_medium: int, raf_inhibitor: int, ras_gtpase_inhibitor: int, retinoid_receptor_agonist: int, retinoid_receptor_antagonist: int, rho_associated_kinase_inhibitor: int, ribonucleoside_reductase_inhibitor: int, rna_polymerase_inhibitor: int, serotonin_receptor_agonist: int, serotonin_receptor_antagonist: int, serotonin_reuptake_inhibitor: int, sigma_receptor_agonist: int, sigma_receptor_antagonist: int, smoothened_receptor_antagonist: int, sodium_channel_inhibitor: int, sphingosine_receptor_agonist: int, src_inhibitor: int, steroid: int, syk_inhibitor: int, tachykinin_antagonist: int, tgf-beta_receptor_inhibitor: int, thrombin_inhibitor: int, thymidylate_synthase_inhibitor: int, tlr_agonist: int, tlr_antagonist: int, tnf_inhibitor: int, topoisomerase_inhibitor: int, transient_receptor_potential_channel_antagonist: int, tropomyosin_receptor_kinase_inhibitor: int, trpv_agonist: int, trpv_antagonist: int, tubulin_inhibitor: int, tyrosine_kinase_inhibitor: int, ubiquitin_specific_protease_inhibitor: int, vegfr_inhibitor: int, vitamin_b: int, vitamin_d_receptor_agonist: int, wnt_inhibitor: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train test split\n",
    "(train, validation) = final_train.randomSplit([0.8, 0.2], 16)\n",
    "\n",
    "train.cache()\n",
    "validation.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifier:\n",
    "    \n",
    "    def __init__(self, clf, labels, feature_col, prob_col='probability'):\n",
    "\n",
    "        self.clf = clf\n",
    "        self.labels = labels\n",
    "        self.feature_col = feature_col\n",
    "        self.prob_col = prob_col\n",
    "        self._trained_clfs = []\n",
    "\n",
    "    def fit(self, train):\n",
    "        self._trained_clfs = [self.clf(labelCol=label, featuresCol=self.feature_col).fit(train) \n",
    "                              for label in self.labels]\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_test):\n",
    "        secondelement = udf(lambda v:float(v[1]),FloatType())\n",
    "        other_cols = ['sig_id'] # need to change if have time\n",
    "        clf = self._trained_clfs[0]\n",
    "        predictions = clf.transform(x_test)\n",
    "        res = predictions.select(*other_cols, secondelement(self.prob_col).alias(self.labels[0]))\n",
    "        for i in range(1, len(self._trained_clfs)):\n",
    "            clf = self._trained_clfs[i]\n",
    "            predictions = clf.transform(x_test)\n",
    "            prob_class = predictions.select(*other_cols,secondelement(self.prob_col).alias(self.labels[i]))\n",
    "            res = res.join(prob_class, other_cols, \"inner\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is just for test. you can deleted after optimize feature engineering\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data\n",
    "train_feature_df = train_df.drop(*['cp_type', 'cp_time','cp_dose'])\n",
    "\n",
    "# assemble features\n",
    "assemble_cols = train_feature_df.drop(sample_id).columns\n",
    "vector_assember_train = VectorAssembler(inputCols=assemble_cols, outputCol='features')\n",
    "train_feature_df = vector_assember_train.transform(train_feature_df).select(sample_id, 'features')\n",
    "train_feature_df = train_feature_df.select(sample_id, 'features')\n",
    "\n",
    "# join features with all targets\n",
    "final_train = train_feature_df.join(target_df, sample_id, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e228a169-dea5-4254-8605-0f7baed953ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------+----------------------+--------------+\n",
      "|      sig_id|5-alpha_reductase_inhibitor|11-beta-hsd1_inhibitor|acat_inhibitor|\n",
      "+------------+---------------------------+----------------------+--------------+\n",
      "|id_000644bb2|               0.0012206286|                   0.0|  1.9825535E-4|\n",
      "|id_000779bfc|                6.932409E-5|                   0.0|  0.0019933556|\n",
      "|id_000a6266a|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_0015fd391|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_001626bd3|                6.932409E-5|           0.007125891|  1.9825535E-4|\n",
      "|id_001bd861f|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_0020d0484|                6.932409E-5|           7.797271E-4|  1.9825535E-4|\n",
      "|id_00224bf20|                6.932409E-5|           7.797271E-4|  1.9825535E-4|\n",
      "|id_0023f063e|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_002452c7e|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_0025c5949|               0.0012206286|                   0.0|  1.9825535E-4|\n",
      "|id_00289dc6e|                6.932409E-5|           7.797271E-4|  1.9825535E-4|\n",
      "|id_002d31e2c|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_002e08ff8|               0.0012206286|                   0.0|  1.9825535E-4|\n",
      "|id_002fb9c19|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "|id_0031dd989|                6.932409E-5|                   0.0|   0.017857144|\n",
      "|id_003603254|                6.932409E-5|           7.797271E-4|  0.0017559263|\n",
      "|id_0036b0342|               0.0012206286|                   0.0|  1.9825535E-4|\n",
      "|id_003b43239|                6.932409E-5|                   0.0|  0.0019933556|\n",
      "|id_003d4b6f0|                6.932409E-5|                   0.0|  1.9825535E-4|\n",
      "+------------+---------------------------+----------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "labels = target_df.drop(sample_id).columns[:3]\n",
    "\n",
    "clf = MultiLabelClassifier(DecisionTreeClassifier, labels, 'features').fit(train)\n",
    "res = clf.transform(train)\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y, y_hat, join_id):\n",
    "    import math\n",
    "    target_cols = y.drop(join_id).columns\n",
    "    @udf('double')\n",
    "    def loss(t, p):\n",
    "        cut = 10**(-15)\n",
    "        p = max(min(p, 1-cut),cut)\n",
    "        return t * math.log(p) + (1 - t) * math.log(1 - p)\n",
    "    name = target_cols[0]\n",
    "    df = y.select(join_id, col(name).alias('target')).join(\n",
    "            y_hat.select(join_id, col(name).alias('predict')),\n",
    "            join_id,\n",
    "            'inner'\n",
    "        )\n",
    "    df = df.withColumn('log_loss', loss('target', 'predict'))\n",
    "    df = df.drop('target', 'predict')\n",
    "    for name in target_cols[1:]:\n",
    "        tmp = y.select(join_id, col(name).alias('target')).join(\n",
    "            y_hat.select(join_id, col(name).alias('predict')),\n",
    "            join_id,\n",
    "            'inner'\n",
    "        )\n",
    "        df = df.join(tmp, join_id, 'inner')\n",
    "        df = df.withColumn('log_loss', col('log_loss')+loss('target', 'predict'))\n",
    "        df = df.drop('target', 'predict')\n",
    "    res = df.select((-_mean(col('log_loss'))).alias('score'))\n",
    "    return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               score|\n",
      "+--------------------+\n",
      "|0.009764208357963494|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = target_df.select(*[sample_id]+labels[:3])\n",
    "score(y, res, sample_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse vector to integer approach"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "project",
   "notebookOrigID": 3148343239384131,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
