{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate memory\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.3 (default, Jul  2 2020 17:30:36)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "import findspark\n",
    "import re\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:\\\\Users\\\\msi\\\\Desktop\\\\spark\\\\spark-3.0.1-bin-hadoop3.2'\n",
    "findspark.init()\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, VectorSlicer, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes, DecisionTreeClassifier\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, lit, udf, mean as _mean, isnan, sum as _sum, log as _log, when\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import FloatType, ArrayType, IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train, Target, Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_features.csv', header=True, inferSchema=True)\n",
    "target_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_targets_scored.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\test_features.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, cp_type: string, cp_time: int, cp_dose: string, g-0: double, g-1: double, g-2: double, g-3: double, g-4: double, g-5: double, g-6: double, g-7: double, g-8: double, g-9: double, g-10: double, g-11: double, g-12: double, g-13: double, g-14: double, g-15: double, g-16: double, g-17: double, g-18: double, g-19: double, g-20: double, g-21: double, g-22: double, g-23: double, g-24: double, g-25: double, g-26: double, g-27: double, g-28: double, g-29: double, g-30: double, g-31: double, g-32: double, g-33: double, g-34: double, g-35: double, g-36: double, g-37: double, g-38: double, g-39: double, g-40: double, g-41: double, g-42: double, g-43: double, g-44: double, g-45: double, g-46: double, g-47: double, g-48: double, g-49: double, g-50: double, g-51: double, g-52: double, g-53: double, g-54: double, g-55: double, g-56: double, g-57: double, g-58: double, g-59: double, g-60: double, g-61: double, g-62: double, g-63: double, g-64: double, g-65: double, g-66: double, g-67: double, g-68: double, g-69: double, g-70: double, g-71: double, g-72: double, g-73: double, g-74: double, g-75: double, g-76: double, g-77: double, g-78: double, g-79: double, g-80: double, g-81: double, g-82: double, g-83: double, g-84: double, g-85: double, g-86: double, g-87: double, g-88: double, g-89: double, g-90: double, g-91: double, g-92: double, g-93: double, g-94: double, g-95: double, g-96: double, g-97: double, g-98: double, g-99: double, g-100: double, g-101: double, g-102: double, g-103: double, g-104: double, g-105: double, g-106: double, g-107: double, g-108: double, g-109: double, g-110: double, g-111: double, g-112: double, g-113: double, g-114: double, g-115: double, g-116: double, g-117: double, g-118: double, g-119: double, g-120: double, g-121: double, g-122: double, g-123: double, g-124: double, g-125: double, g-126: double, g-127: double, g-128: double, g-129: double, g-130: double, g-131: double, g-132: double, g-133: double, g-134: double, g-135: double, g-136: double, g-137: double, g-138: double, g-139: double, g-140: double, g-141: double, g-142: double, g-143: double, g-144: double, g-145: double, g-146: double, g-147: double, g-148: double, g-149: double, g-150: double, g-151: double, g-152: double, g-153: double, g-154: double, g-155: double, g-156: double, g-157: double, g-158: double, g-159: double, g-160: double, g-161: double, g-162: double, g-163: double, g-164: double, g-165: double, g-166: double, g-167: double, g-168: double, g-169: double, g-170: double, g-171: double, g-172: double, g-173: double, g-174: double, g-175: double, g-176: double, g-177: double, g-178: double, g-179: double, g-180: double, g-181: double, g-182: double, g-183: double, g-184: double, g-185: double, g-186: double, g-187: double, g-188: double, g-189: double, g-190: double, g-191: double, g-192: double, g-193: double, g-194: double, g-195: double, g-196: double, g-197: double, g-198: double, g-199: double, g-200: double, g-201: double, g-202: double, g-203: double, g-204: double, g-205: double, g-206: double, g-207: double, g-208: double, g-209: double, g-210: double, g-211: double, g-212: double, g-213: double, g-214: double, g-215: double, g-216: double, g-217: double, g-218: double, g-219: double, g-220: double, g-221: double, g-222: double, g-223: double, g-224: double, g-225: double, g-226: double, g-227: double, g-228: double, g-229: double, g-230: double, g-231: double, g-232: double, g-233: double, g-234: double, g-235: double, g-236: double, g-237: double, g-238: double, g-239: double, g-240: double, g-241: double, g-242: double, g-243: double, g-244: double, g-245: double, g-246: double, g-247: double, g-248: double, g-249: double, g-250: double, g-251: double, g-252: double, g-253: double, g-254: double, g-255: double, g-256: double, g-257: double, g-258: double, g-259: double, g-260: double, g-261: double, g-262: double, g-263: double, g-264: double, g-265: double, g-266: double, g-267: double, g-268: double, g-269: double, g-270: double, g-271: double, g-272: double, g-273: double, g-274: double, g-275: double, g-276: double, g-277: double, g-278: double, g-279: double, g-280: double, g-281: double, g-282: double, g-283: double, g-284: double, g-285: double, g-286: double, g-287: double, g-288: double, g-289: double, g-290: double, g-291: double, g-292: double, g-293: double, g-294: double, g-295: double, g-296: double, g-297: double, g-298: double, g-299: double, g-300: double, g-301: double, g-302: double, g-303: double, g-304: double, g-305: double, g-306: double, g-307: double, g-308: double, g-309: double, g-310: double, g-311: double, g-312: double, g-313: double, g-314: double, g-315: double, g-316: double, g-317: double, g-318: double, g-319: double, g-320: double, g-321: double, g-322: double, g-323: double, g-324: double, g-325: double, g-326: double, g-327: double, g-328: double, g-329: double, g-330: double, g-331: double, g-332: double, g-333: double, g-334: double, g-335: double, g-336: double, g-337: double, g-338: double, g-339: double, g-340: double, g-341: double, g-342: double, g-343: double, g-344: double, g-345: double, g-346: double, g-347: double, g-348: double, g-349: double, g-350: double, g-351: double, g-352: double, g-353: double, g-354: double, g-355: double, g-356: double, g-357: double, g-358: double, g-359: double, g-360: double, g-361: double, g-362: double, g-363: double, g-364: double, g-365: double, g-366: double, g-367: double, g-368: double, g-369: double, g-370: double, g-371: double, g-372: double, g-373: double, g-374: double, g-375: double, g-376: double, g-377: double, g-378: double, g-379: double, g-380: double, g-381: double, g-382: double, g-383: double, g-384: double, g-385: double, g-386: double, g-387: double, g-388: double, g-389: double, g-390: double, g-391: double, g-392: double, g-393: double, g-394: double, g-395: double, g-396: double, g-397: double, g-398: double, g-399: double, g-400: double, g-401: double, g-402: double, g-403: double, g-404: double, g-405: double, g-406: double, g-407: double, g-408: double, g-409: double, g-410: double, g-411: double, g-412: double, g-413: double, g-414: double, g-415: double, g-416: double, g-417: double, g-418: double, g-419: double, g-420: double, g-421: double, g-422: double, g-423: double, g-424: double, g-425: double, g-426: double, g-427: double, g-428: double, g-429: double, g-430: double, g-431: double, g-432: double, g-433: double, g-434: double, g-435: double, g-436: double, g-437: double, g-438: double, g-439: double, g-440: double, g-441: double, g-442: double, g-443: double, g-444: double, g-445: double, g-446: double, g-447: double, g-448: double, g-449: double, g-450: double, g-451: double, g-452: double, g-453: double, g-454: double, g-455: double, g-456: double, g-457: double, g-458: double, g-459: double, g-460: double, g-461: double, g-462: double, g-463: double, g-464: double, g-465: double, g-466: double, g-467: double, g-468: double, g-469: double, g-470: double, g-471: double, g-472: double, g-473: double, g-474: double, g-475: double, g-476: double, g-477: double, g-478: double, g-479: double, g-480: double, g-481: double, g-482: double, g-483: double, g-484: double, g-485: double, g-486: double, g-487: double, g-488: double, g-489: double, g-490: double, g-491: double, g-492: double, g-493: double, g-494: double, g-495: double, g-496: double, g-497: double, g-498: double, g-499: double, g-500: double, g-501: double, g-502: double, g-503: double, g-504: double, g-505: double, g-506: double, g-507: double, g-508: double, g-509: double, g-510: double, g-511: double, g-512: double, g-513: double, g-514: double, g-515: double, g-516: double, g-517: double, g-518: double, g-519: double, g-520: double, g-521: double, g-522: double, g-523: double, g-524: double, g-525: double, g-526: double, g-527: double, g-528: double, g-529: double, g-530: double, g-531: double, g-532: double, g-533: double, g-534: double, g-535: double, g-536: double, g-537: double, g-538: double, g-539: double, g-540: double, g-541: double, g-542: double, g-543: double, g-544: double, g-545: double, g-546: double, g-547: double, g-548: double, g-549: double, g-550: double, g-551: double, g-552: double, g-553: double, g-554: double, g-555: double, g-556: double, g-557: double, g-558: double, g-559: double, g-560: double, g-561: double, g-562: double, g-563: double, g-564: double, g-565: double, g-566: double, g-567: double, g-568: double, g-569: double, g-570: double, g-571: double, g-572: double, g-573: double, g-574: double, g-575: double, g-576: double, g-577: double, g-578: double, g-579: double, g-580: double, g-581: double, g-582: double, g-583: double, g-584: double, g-585: double, g-586: double, g-587: double, g-588: double, g-589: double, g-590: double, g-591: double, g-592: double, g-593: double, g-594: double, g-595: double, g-596: double, g-597: double, g-598: double, g-599: double, g-600: double, g-601: double, g-602: double, g-603: double, g-604: double, g-605: double, g-606: double, g-607: double, g-608: double, g-609: double, g-610: double, g-611: double, g-612: double, g-613: double, g-614: double, g-615: double, g-616: double, g-617: double, g-618: double, g-619: double, g-620: double, g-621: double, g-622: double, g-623: double, g-624: double, g-625: double, g-626: double, g-627: double, g-628: double, g-629: double, g-630: double, g-631: double, g-632: double, g-633: double, g-634: double, g-635: double, g-636: double, g-637: double, g-638: double, g-639: double, g-640: double, g-641: double, g-642: double, g-643: double, g-644: double, g-645: double, g-646: double, g-647: double, g-648: double, g-649: double, g-650: double, g-651: double, g-652: double, g-653: double, g-654: double, g-655: double, g-656: double, g-657: double, g-658: double, g-659: double, g-660: double, g-661: double, g-662: double, g-663: double, g-664: double, g-665: double, g-666: double, g-667: double, g-668: double, g-669: double, g-670: double, g-671: double, g-672: double, g-673: double, g-674: double, g-675: double, g-676: double, g-677: double, g-678: double, g-679: double, g-680: double, g-681: double, g-682: double, g-683: double, g-684: double, g-685: double, g-686: double, g-687: double, g-688: double, g-689: double, g-690: double, g-691: double, g-692: double, g-693: double, g-694: double, g-695: double, g-696: double, g-697: double, g-698: double, g-699: double, g-700: double, g-701: double, g-702: double, g-703: double, g-704: double, g-705: double, g-706: double, g-707: double, g-708: double, g-709: double, g-710: double, g-711: double, g-712: double, g-713: double, g-714: double, g-715: double, g-716: double, g-717: double, g-718: double, g-719: double, g-720: double, g-721: double, g-722: double, g-723: double, g-724: double, g-725: double, g-726: double, g-727: double, g-728: double, g-729: double, g-730: double, g-731: double, g-732: double, g-733: double, g-734: double, g-735: double, g-736: double, g-737: double, g-738: double, g-739: double, g-740: double, g-741: double, g-742: double, g-743: double, g-744: double, g-745: double, g-746: double, g-747: double, g-748: double, g-749: double, g-750: double, g-751: double, g-752: double, g-753: double, g-754: double, g-755: double, g-756: double, g-757: double, g-758: double, g-759: double, g-760: double, g-761: double, g-762: double, g-763: double, g-764: double, g-765: double, g-766: double, g-767: double, g-768: double, g-769: double, g-770: double, g-771: double, c-0: double, c-1: double, c-2: double, c-3: double, c-4: double, c-5: double, c-6: double, c-7: double, c-8: double, c-9: double, c-10: double, c-11: double, c-12: double, c-13: double, c-14: double, c-15: double, c-16: double, c-17: double, c-18: double, c-19: double, c-20: double, c-21: double, c-22: double, c-23: double, c-24: double, c-25: double, c-26: double, c-27: double, c-28: double, c-29: double, c-30: double, c-31: double, c-32: double, c-33: double, c-34: double, c-35: double, c-36: double, c-37: double, c-38: double, c-39: double, c-40: double, c-41: double, c-42: double, c-43: double, c-44: double, c-45: double, c-46: double, c-47: double, c-48: double, c-49: double, c-50: double, c-51: double, c-52: double, c-53: double, c-54: double, c-55: double, c-56: double, c-57: double, c-58: double, c-59: double, c-60: double, c-61: double, c-62: double, c-63: double, c-64: double, c-65: double, c-66: double, c-67: double, c-68: double, c-69: double, c-70: double, c-71: double, c-72: double, c-73: double, c-74: double, c-75: double, c-76: double, c-77: double, c-78: double, c-79: double, c-80: double, c-81: double, c-82: double, c-83: double, c-84: double, c-85: double, c-86: double, c-87: double, c-88: double, c-89: double, c-90: double, c-91: double, c-92: double, c-93: double, c-94: double, c-95: double, c-96: double, c-97: double, c-98: double, c-99: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "target_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_features(df, cat_features):\n",
    "\n",
    "  indexed_cols = [''.join([col_name, '_indexed']) for col_name in cat_features]\n",
    "  encoded_cols = [''.join([col_name, '_encoded']) for col_name in cat_features]\n",
    "  string_indexers = [StringIndexer(inputCol=cat_features[i], outputCol=indexed_cols[i]) for i in range(len(cat_features))]\n",
    "    \n",
    "  encoder = OneHotEncoder(inputCols=indexed_cols, outputCols=encoded_cols)\n",
    "  \n",
    "  pipline = Pipeline(stages=string_indexers + [encoder])\n",
    "  \n",
    "  encoded_df = pipline.fit(df).transform(df)\n",
    "  encoded_df = encoded_df.drop(*indexed_cols + cat_features)\n",
    "\n",
    "  return encoded_df\n",
    "\n",
    "def normalize_features(df, cols, normalizer, output_cols, if_drop=True):\n",
    "    \n",
    "  normalizer_lst = []\n",
    "  vectorized_cols = []\n",
    "  vector_assembers = []\n",
    "  \n",
    "  if isinstance(cols, list):\n",
    "    cols = {'cols': cols}\n",
    "  \n",
    "  if isinstance(output_cols, str):\n",
    "    output_cols = {'cols': output_cols}\n",
    "  \n",
    "  for k, v in cols.items():\n",
    "    \n",
    "    temp_normalizer = normalizer.copy()\n",
    "    vectorized_col = ''.join([output_cols[k], '_v'])\n",
    "    vector_assember = VectorAssembler(inputCols=v, outputCol=vectorized_col)\n",
    "    \n",
    "    temp_normalizer.setInputCol(vectorized_col)\n",
    "    temp_normalizer.setOutputCol(output_cols[k])\n",
    "    \n",
    "    normalizer_lst.append(temp_normalizer)\n",
    "    vectorized_cols.append(vectorized_col)\n",
    "    vector_assembers.append(vector_assember)\n",
    "  \n",
    "  pipline = Pipeline(stages=vector_assembers + normalizer_lst)\n",
    "  normalized_df = pipline.fit(df).transform(df).drop(*vectorized_cols)\n",
    "  \n",
    "  if if_drop:\n",
    "    \n",
    "    for k, v in cols.items():\n",
    "      \n",
    "      normalized_df = normalized_df.drop(v)\n",
    "  \n",
    "  return normalized_df\n",
    "\n",
    "def add_pca_features(df, g_cols, c_cols, k=40):\n",
    "  \n",
    "  ## normalize g-col and c-col\n",
    "  std_scaler = StandardScaler(withMean=True)\n",
    "  \n",
    "  input_cols = {\n",
    "    'g_cols': g_cols, \n",
    "    'c_cols': c_cols}\n",
    "  \n",
    "  output_cols = {\n",
    "    'g_cols': 'g_normalized', \n",
    "    'c_cols': 'c_normalized'}\n",
    "  \n",
    "  normalized_df = normalize_features(df, input_cols, std_scaler, output_cols, if_drop=False)\n",
    "  \n",
    "  ## perform PCA on g-cols and c-cols\n",
    "  g_col_pca = PCA(k=k, inputCol='g_normalized', outputCol='g_col_pca')\n",
    "  c_col_pca = PCA(k=k, inputCol='c_normalized', outputCol='c_col_pca')\n",
    "  \n",
    "  pipeline = Pipeline(stages=[g_col_pca, c_col_pca])\n",
    "  pca_df = pipeline.fit(normalized_df).transform(normalized_df)\n",
    "  \n",
    "  return pca_df\n",
    "  \n",
    "def add_stats_features(df, g_cols, c_cols):\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_sum(*lst):\n",
    "\n",
    "    return sum(lst)\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_mean(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst)\n",
    "\n",
    "    return s / n\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_var(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst) / n\n",
    "    total = 0\n",
    "\n",
    "    for x in lst:\n",
    "\n",
    "      total += (x - s)**2 \n",
    "\n",
    "    return total / n\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_min(*lst):\n",
    "    \n",
    "    return min(lst)\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_max(*lst):\n",
    "    \n",
    "    return max(lst)\n",
    "  \n",
    "  stats_dict = {\n",
    "    'min_stats': cols_min,\n",
    "    'max_stats': cols_max,\n",
    "    'var_stats': cols_var,\n",
    "    'mean_stats': cols_mean,\n",
    "    'sum_stats': cols_sum\n",
    "  }\n",
    "  \n",
    "  for name, func in stats_dict.items():\n",
    "    \n",
    "    df = df.withColumn(''.join(['g_cols_', name]), func(*[col(g_col) for g_col in g_cols]))\n",
    "    df = df.withColumn(''.join(['c_cols_', name]), func(*[col(c_col) for c_col in c_cols]))\n",
    "  \n",
    "  return df\n",
    "\n",
    "def add_kmeans_features(df, g_cols, c_cols, k=2, num_iter=10):\n",
    "  \n",
    "  kmeans_g = KMeans(k=k, featuresCol=g_cols, predictionCol='g_col_k_mean', seed=16)\n",
    "  kmeans_c = KMeans(k=k, featuresCol=c_cols, predictionCol='c_col_k_mean', seed=16)\n",
    "  \n",
    "  kmeans_df = kmeans_g.fit(df).transform(df)\n",
    "  kmeans_df = kmeans_c.fit(kmeans_df).transform(kmeans_df)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def feature_engineering(df, num_cluster=2, num_comp=40, num_iter=10):\n",
    "  \n",
    "  ## get g-col and c-col\n",
    "  g_cols = list(filter(lambda v: re.match('g-.+', v), df.columns))\n",
    "  c_cols = list(filter(lambda v: re.match('c-.+', v), df.columns))\n",
    "  \n",
    "  ## PCA\n",
    "  pca_df = add_pca_features(df, g_cols, c_cols, num_comp)\n",
    "\n",
    "  ## stats features on g and c cols\n",
    "  stats_df = add_stats_features(pca_df, g_cols, c_cols)\n",
    "  \n",
    "  ## add k-means features\n",
    "  kmeans_df = add_kmeans_features(stats_df, g_cols='g_normalized', c_cols='c_normalized', k=num_cluster, num_iter=num_iter)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def get_correlation(df, threshold=0.95, feature_col='all_features', plot=False):\n",
    "    \n",
    "    from pyspark.ml.stat import Correlation\n",
    "    from pyspark.sql.types import FloatType, ArrayType, IntegerType\n",
    "    import numpy as np\n",
    "    \n",
    "    r1 = Correlation.corr(df, feature_col)\n",
    "    r1 = r1.select(f'pearson({feature_col})').collect()[0][f'pearson({feature_col})'].toArray().tolist()\n",
    "    \n",
    "    ## if plot == True, plot the correlation plot using seaborn\n",
    "    if plot:\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        cor_plt_array = np.array(r1)\n",
    "        sns.heatmap(cor_plt_array)\n",
    "        \n",
    "    ## extract values from dense matrix to a new dataframe\n",
    "    r1 = spark.createDataFrame(r1)\n",
    "    gv = sc.broadcast(threshold)\n",
    "    va = VectorAssembler(inputCols=r1.columns, outputCol='features')\n",
    "    r1 = va.transform(r1)\n",
    "    \n",
    "    ## find groups of features that have high correlation values\n",
    "    def find_element(row, gv):\n",
    "    \n",
    "        lst = []\n",
    "\n",
    "        for i in range(len(row)):\n",
    "\n",
    "            if (row[i] >= gv.value) | (row[i] <= -gv.value):\n",
    "                lst.append(i)\n",
    "        \n",
    "        if len(lst) == 1:\n",
    "            lst = []\n",
    "            \n",
    "        return lst\n",
    "\n",
    "    m = udf(lambda x: find_element(x, gv), ArrayType(IntegerType()))\n",
    "    \n",
    "    return r1.withColumn('correlated_features', m('features')).select('correlated_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add indicator column to both train and test so we can combine them later\n",
    "train_df = train_df.withColumn('is_test', lit(0))\n",
    "test_df = test_df.withColumn('is_test', lit(1))\n",
    "\n",
    "## Combine train and test df\n",
    "full_df = train_df.union(test_df)\n",
    "\n",
    "## encode features\n",
    "target_cols = ['cp_type', 'cp_dose']\n",
    "encoded_df = encode_cat_features(full_df, target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Multilabel Classifier\n",
    "class MultiLabelClassifier:\n",
    "    \n",
    "    def __init__(self, clf, labels, feature_col,  \n",
    "                 hyperparameters=[], \n",
    "                 predict_col=['probability','prediction'],\n",
    "                 method=lambda prob_col, pred_col: float(pred_col if len(prob_col) == 1 else prob_col[1])):\n",
    "        '''\n",
    "        Initialize a multilabelclassifier\n",
    "        clf: the model to use\n",
    "        labels: a list of labels to predict\n",
    "        feature_col: the feature column\n",
    "        predict_col: the prediction column where the prediction is located\n",
    "        hyperparameters: all optional hyperparameters that can tune\n",
    "        method: a method of how to get the final prediction for one class\n",
    "        '''\n",
    "        self.clf = clf\n",
    "        self.labels = labels\n",
    "        self.feature_col = feature_col\n",
    "        self.predict_col = predict_col\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.method = method\n",
    "        self._trained_clfs = []\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        train.cache()\n",
    "        \n",
    "        for label, hyperparameters in tqdm(zip(self.labels, self.hyperparameters)):\n",
    "            ## add weight col\n",
    "            temp_train = train.withColumn('weight', when(col(label) == 1, 2).otherwise(1))\n",
    "            temp_clf = self.clf(labelCol=label, featuresCol=self.feature_col, weightCol='weight', **hyperparameters).fit(temp_train)\n",
    "            self._trained_clfs.append(temp_clf)\n",
    "            \n",
    "            temp_train.unpersist()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, x_test):\n",
    "        # convert method to udf\n",
    "        get_predict = udf(self.method,FloatType())\n",
    "        #target assembler\n",
    "        va = VectorAssembler(inputCols=self.labels, outputCol='targets')\n",
    "        ## transform this vector self.output_col to an array\n",
    "        select_cols = [self.feature_col, 'targets', 'sig_id']\n",
    "        res = va.transform(x_test).select(*select_cols)\n",
    "        for i, clf in tqdm(enumerate(self._trained_clfs)):\n",
    "            res = clf.transform(res)\n",
    "            new_col = self.labels[i]\n",
    "            res = res.withColumn(new_col, get_predict(*self.predict_col))\n",
    "            select_cols.append(new_col)\n",
    "            res = res.select(*select_cols)\n",
    "        self.res = res\n",
    "        return res.select(*select_cols[2:])\n",
    "    \n",
    "    def score(self):\n",
    "        #target assembler \n",
    "        va = VectorAssembler(inputCols=self.labels, outputCol='predicts')\n",
    "        ## transform this vector self.output_col to an array\n",
    "        df = va.transform(self.res).select('targets', 'predicts')\n",
    "        df = df.withColumn('targets', vector_to_array('targets'))\n",
    "        df = df.withColumn('predicts', vector_to_array('predicts'))\n",
    "        import math\n",
    "        @udf('double')\n",
    "        def log_loss(y, y_hat):\n",
    "            r = 0\n",
    "            cut = 1e-15\n",
    "            for t, p in zip(y, y_hat):\n",
    "                p = max(min(p, 1-cut),cut)\n",
    "                r += t * math.log(p) + (1 - t) * math.log(1 - p)\n",
    "            return r/len(y)\n",
    "        df = df.select(log_loss('targets','predicts').alias('log_loss'))\n",
    "        return df.select((-_mean(col('log_loss'))).alias('score'))\n",
    "    \n",
    "    def param_search_cv(self, train, grid_map, fold_num):\n",
    "        \"\"\"\n",
    "        Perform param search on each binary classifier\n",
    "        \n",
    "        train: the traning set for cross validation\n",
    "        grid_map: the param grid map contains params for tunning\n",
    "        fold_num: number of folds for k fold cross validation\n",
    "        return: a list of dict({param: value}), one dict for each binary classifier\n",
    "        \"\"\"\n",
    "                   \n",
    "        train.cache()\n",
    "        best_params = []\n",
    "        \n",
    "        def _extract_best_params(cv_model):\n",
    "            \"\"\"\n",
    "            extract best params from cross validation output\n",
    "            \n",
    "            cv_model: CrossValidationModel\n",
    "            return: dict({param: value})\n",
    "            \"\"\"\n",
    "            \n",
    "            best_param_dict = {}\n",
    "            scores = cv_model.avgMetrics\n",
    "            best_param = cv_model.getEstimatorParamMaps()[scores.index(min(scores))]\n",
    "\n",
    "            for k, v in best_param.items():\n",
    "                best_param_dict[k.name] = v\n",
    "            \n",
    "            return best_param_dict\n",
    "            \n",
    "        for label in tqdm(self.labels):\n",
    "            \n",
    "            ## for each label: init classifier, evaluator, cross validator\n",
    "            clf = self.clf(labelCol=label, featuresCol=self.feature_col)\n",
    "            evaluator = MultilabelEvaluator(method=self.method, predictionCol=self.predict_col, labelCol=label)\n",
    "            cv = CrossValidator(estimator=clf, estimatorParamMaps=grid_map, numFolds=fold_num, evaluator=evaluator, parallelism=4)\n",
    "            \n",
    "            ## fit the cross validator and  extract best params for this binary classifier\n",
    "            cv_model = cv.fit(train)\n",
    "            best_model = cv_model.bestModel\n",
    "            best_param = _extract_best_params(cv_model)\n",
    "            \n",
    "            ## append best model and append the best params\n",
    "            self._trained_clfs.append(best_model)\n",
    "            best_params.append(best_param) \n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "\n",
    "class MultilabelEvaluator(Evaluator):\n",
    "    \n",
    "    def __init__(self, method, predictionCol=['probability', 'prediction'], labelCol=\"label\"):\n",
    "        \n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.method = method\n",
    "        \n",
    "    def _evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "        calculate log loss for single binary classifier's prediction\n",
    "        \n",
    "        dataset: the dataset contains the output of the binary classifier\n",
    "        return: the log loss score\n",
    "        \"\"\"\n",
    "        ## log loss for single binary classifier\n",
    "        get_predict = udf(self.method,FloatType())\n",
    "        dataset = dataset.withColumn('pred_prob', get_predict(*self.predictionCol))\n",
    "        cut = 1e-15\n",
    "        new_dataset = dataset.select((-col(self.labelCol) * _log(col('pred_prob') + cut) - (1.0 - col(self.labelCol)) * _log(1.0 - col('pred_prob') + cut)).alias('log_loss'))        \n",
    "        score = new_dataset.select(_mean(col('log_loss')).alias('score')).collect()[0]['score']\n",
    "        return score\n",
    "        \n",
    "    def isLargerBetter(self):\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(prob_col, pred_col):\n",
    "    \"\"\"\n",
    "    extract probability of class1 from output probability column prob_col which consists of [prob of class0, prob of class1], if len(prob_col) == 1, \n",
    "    then use the pred_col.\n",
    "    \n",
    "    prob_col: probability column\n",
    "    pred_col: prediction column\n",
    "    return probability of class1\n",
    "    \"\"\"\n",
    "    \n",
    "    ## solve na problem, if len(prob_col) == 1, we use the prediction col\n",
    "    converted_prob_col = prob_col.toArray().tolist()\n",
    "\n",
    "    if len(converted_prob_col) == 1:\n",
    "\n",
    "        return pred_col\n",
    "\n",
    "    else:\n",
    "\n",
    "        return converted_prob_col[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "Only contains the original dataset, after categorical encoding, no new features, no PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full dataset\n",
    "exclude_features = ['sig_id', 'cp_dose', 'cp_type']\n",
    "vector_assember_train = VectorAssembler(inputCols=[col for col in encoded_df.columns if col not in exclude_features], outputCol='features')\n",
    "fe_df = vector_assember_train.transform(encoded_df)\n",
    "\n",
    "## split train, test df\n",
    "fe_train = fe_df.filter(fe_df['is_test'] == 0)\n",
    "final_test = fe_df.filter(fe_df['is_test'] == 1).select(['sig_id', 'features'])\n",
    "\n",
    "## join training target with training features\n",
    "labels = target_df.drop('sig_id').columns\n",
    "final_train = fe_train.join(target_df, ['sig_id']).select(*(['sig_id','features'] + labels))\n",
    "\n",
    "## train test split\n",
    "(cv_train, test) = final_train.randomSplit([0.8, 0.2], 16)\n",
    "(train, validation) = cv_train.randomSplit([0.8, 0.2], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## baseline Decision Tree\n",
    "clf = DecisionTreeClassifier\n",
    "hyperparameters = [{}] * len(labels)\n",
    "method = convert_to_array\n",
    "baseline_dt = MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [20:37,  6.01s/it]\n",
      "206it [01:14,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline score for Decision Tree model is : \n",
      "+-------------------+\n",
      "|              score|\n",
      "+-------------------+\n",
      "|0.04474902213585472|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_dt.fit(train).transform(validation)\n",
    "print(f'The baseline score for Decision Tree model is : ')\n",
    "baseline_dt.score().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## baseline Logsitic Regression\n",
    "clf = LogisticRegression\n",
    "baseline_lr = MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [39:27, 11.50s/it]\n",
      "206it [01:15,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline score for Logistic Regression model is : \n",
      "+-------------------+\n",
      "|              score|\n",
      "+-------------------+\n",
      "|0.12325460344160755|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_lr.fit(train).transform(validation)\n",
    "print('The baseline score for Logistic Regression model is : ')\n",
    "baseline_lr.score().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## baseline Gaussian Naive Bayes\n",
    "clf = NaiveBayes\n",
    "hyperparameters = [{'modelType': 'gaussian'}] * len(labels)\n",
    "baseline_nb = MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [05:29,  1.60s/it]\n",
      "206it [01:07,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline score for Gaussian Naive Bayes model is :\n",
      "+------------------+\n",
      "|             score|\n",
      "+------------------+\n",
      "|14.246018279911636|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_nb.fit(train).transform(validation)\n",
    "print(f'The baseline score for Gaussian Naive Bayes model is :')\n",
    "baseline_nb.score().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model after feature engineering and feature selection\n",
    "\n",
    "Decision Tree with PCA + Feature engineering + Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|correlated_features|\n",
      "+-------------------+\n",
      "|            [0, 14]|\n",
      "|                 []|\n",
      "|           [16, 18]|\n",
      "|        [5, 17, 19]|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## feature engineering\n",
    "fe_df = feature_engineering(encoded_df, num_comp=5, num_iter=5)\n",
    "\n",
    "## select all the feature columns\n",
    "\n",
    "pca_cols = list(filter(lambda v: re.match('.+_pca', v), fe_df.columns))\n",
    "stats_cols = list(filter(lambda v: re.match('.+_stats', v), fe_df.columns))\n",
    "k_means_cols = list(filter(lambda v: re.match('.+_k_mean', v), fe_df.columns))\n",
    "cat_cols = list(filter(lambda v: re.match('.+_encoded', v), fe_df.columns)) + ['cp_time']\n",
    "all_cols = pca_cols + stats_cols + k_means_cols + cat_cols\n",
    "## stack them to a single feature vector\n",
    "vector_assember_train = VectorAssembler(inputCols=all_cols, outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df)\n",
    "\n",
    "## Check their correlation\n",
    "corr_df = get_correlation(fe_df, 0.96, plot=True)\n",
    "corr_df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c_cols_max_stats'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 40 PCA columns\n",
    "## looking for position 3, 6, 7, 8\n",
    "(stats_cols + k_means_cols + cat_cols)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g_cols_mean_stats'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c_cols_mean_stats'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g_cols_sum_stats'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features = ['c_cols_max_stats', 'g_cols_mean_stats', 'c_cols_mean_stats', 'g_cols_sum_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stack them to a single feature vector again, and remove those features with high correlation\n",
    "vector_assember_train = VectorAssembler(inputCols=[col for col in all_cols if col not in high_corr_features], outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df.drop('all_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize all the features\n",
    "normalizer = StandardScaler(withMean=True)\n",
    "cols = ['all_features']\n",
    "output_cols = 'features'\n",
    "fe_df = normalize_features(fe_df, cols, normalizer, output_cols, if_drop=False)\n",
    "\n",
    "## split train, test df\n",
    "fe_train = fe_df.filter(fe_df['is_test'] == 0)\n",
    "final_test = fe_df.filter(fe_df['is_test'] == 1).select(['sig_id', 'features'])\n",
    "\n",
    "## join training target with training features\n",
    "labels = target_df.drop('sig_id').columns\n",
    "final_train = fe_train.join(target_df, ['sig_id']).select(*(['sig_id','features'] + labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[correlated_features: array<int>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train test split\n",
    "(cv_train, test) = final_train.randomSplit([0.8, 0.2], 16)\n",
    "(train, validation) = cv_train.randomSplit([0.8, 0.2], 16)\n",
    "\n",
    "train.cache()\n",
    "cv_train.cache()\n",
    "validation.cache()\n",
    "test.cache()\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "target_df.unpersist()\n",
    "fe_train.unpersist()\n",
    "fe_df.unpersist()\n",
    "final_train.unpersist()\n",
    "encoded_df.unpersist()\n",
    "corr_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering + Feature selection Decision Tree\n",
    "clf = DecisionTreeClassifier\n",
    "hyperparameters = [{}] * len(labels)\n",
    "method = convert_to_array\n",
    "dt = MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [05:46,  1.68s/it]\n",
      "206it [01:33,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score after feature engineering + feature selection for Decision Tree model is : \n",
      "+-------------------+\n",
      "|              score|\n",
      "+-------------------+\n",
      "|0.03333891635429162|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(train).transform(validation)\n",
    "print('The score after feature engineering + feature selection for Decision Tree model is : ')\n",
    "dt.score().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model after tuning\n",
    "\n",
    "Decision Tree, after hyperparameter tuning\n",
    "\n",
    "Best params are {'maxDepth': 2, 'impurity': 'gini', 'maxBins': 8, 'minInstancesPerNode': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## hyperparameter tuning\n",
    "clf = DecisionTreeClassifier\n",
    "grid_map = ParamGridBuilder().addGrid(clf().maxDepth, [2, 4, 7]).addGrid(clf().impurity, ['gini', 'entropy']).addGrid(clf().maxBins, [8, 32, 64]).addGrid(clf().minInstancesPerNode, [1, 8, 16]).build()\n",
    "method = convert_to_array\n",
    "dt = MultiLabelClassifier(clf, labels, 'features', method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_params = dt.param_search_cv(cv_train, grid_map, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier\n",
    "method = convert_to_array\n",
    "hyperparameters = [{'maxDepth': 2, 'impurity': 'gini', 'maxBins': 8, 'minInstancesPerNode': 1}] * len(labels)\n",
    "final_dt = MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [04:39,  1.36s/it]\n",
      "206it [01:33,  2.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, 5-alpha_reductase_inhibitor: float, 11-beta-hsd1_inhibitor: float, acat_inhibitor: float, acetylcholine_receptor_agonist: float, acetylcholine_receptor_antagonist: float, acetylcholinesterase_inhibitor: float, adenosine_receptor_agonist: float, adenosine_receptor_antagonist: float, adenylyl_cyclase_activator: float, adrenergic_receptor_agonist: float, adrenergic_receptor_antagonist: float, akt_inhibitor: float, aldehyde_dehydrogenase_inhibitor: float, alk_inhibitor: float, ampk_activator: float, analgesic: float, androgen_receptor_agonist: float, androgen_receptor_antagonist: float, anesthetic_-_local: float, angiogenesis_inhibitor: float, angiotensin_receptor_antagonist: float, anti-inflammatory: float, antiarrhythmic: float, antibiotic: float, anticonvulsant: float, antifungal: float, antihistamine: float, antimalarial: float, antioxidant: float, antiprotozoal: float, antiviral: float, apoptosis_stimulant: float, aromatase_inhibitor: float, atm_kinase_inhibitor: float, atp-sensitive_potassium_channel_antagonist: float, atp_synthase_inhibitor: float, atpase_inhibitor: float, atr_kinase_inhibitor: float, aurora_kinase_inhibitor: float, autotaxin_inhibitor: float, bacterial_30s_ribosomal_subunit_inhibitor: float, bacterial_50s_ribosomal_subunit_inhibitor: float, bacterial_antifolate: float, bacterial_cell_wall_synthesis_inhibitor: float, bacterial_dna_gyrase_inhibitor: float, bacterial_dna_inhibitor: float, bacterial_membrane_integrity_inhibitor: float, bcl_inhibitor: float, bcr-abl_inhibitor: float, benzodiazepine_receptor_agonist: float, beta_amyloid_inhibitor: float, bromodomain_inhibitor: float, btk_inhibitor: float, calcineurin_inhibitor: float, calcium_channel_blocker: float, cannabinoid_receptor_agonist: float, cannabinoid_receptor_antagonist: float, carbonic_anhydrase_inhibitor: float, casein_kinase_inhibitor: float, caspase_activator: float, catechol_o_methyltransferase_inhibitor: float, cc_chemokine_receptor_antagonist: float, cck_receptor_antagonist: float, cdk_inhibitor: float, chelating_agent: float, chk_inhibitor: float, chloride_channel_blocker: float, cholesterol_inhibitor: float, cholinergic_receptor_antagonist: float, coagulation_factor_inhibitor: float, corticosteroid_agonist: float, cyclooxygenase_inhibitor: float, cytochrome_p450_inhibitor: float, dihydrofolate_reductase_inhibitor: float, dipeptidyl_peptidase_inhibitor: float, diuretic: float, dna_alkylating_agent: float, dna_inhibitor: float, dopamine_receptor_agonist: float, dopamine_receptor_antagonist: float, egfr_inhibitor: float, elastase_inhibitor: float, erbb2_inhibitor: float, estrogen_receptor_agonist: float, estrogen_receptor_antagonist: float, faah_inhibitor: float, farnesyltransferase_inhibitor: float, fatty_acid_receptor_agonist: float, fgfr_inhibitor: float, flt3_inhibitor: float, focal_adhesion_kinase_inhibitor: float, free_radical_scavenger: float, fungal_squalene_epoxidase_inhibitor: float, gaba_receptor_agonist: float, gaba_receptor_antagonist: float, gamma_secretase_inhibitor: float, glucocorticoid_receptor_agonist: float, glutamate_inhibitor: float, glutamate_receptor_agonist: float, glutamate_receptor_antagonist: float, gonadotropin_receptor_agonist: float, gsk_inhibitor: float, hcv_inhibitor: float, hdac_inhibitor: float, histamine_receptor_agonist: float, histamine_receptor_antagonist: float, histone_lysine_demethylase_inhibitor: float, histone_lysine_methyltransferase_inhibitor: float, hiv_inhibitor: float, hmgcr_inhibitor: float, hsp_inhibitor: float, igf-1_inhibitor: float, ikk_inhibitor: float, imidazoline_receptor_agonist: float, immunosuppressant: float, insulin_secretagogue: float, insulin_sensitizer: float, integrin_inhibitor: float, jak_inhibitor: float, kit_inhibitor: float, laxative: float, leukotriene_inhibitor: float, leukotriene_receptor_antagonist: float, lipase_inhibitor: float, lipoxygenase_inhibitor: float, lxr_agonist: float, mdm_inhibitor: float, mek_inhibitor: float, membrane_integrity_inhibitor: float, mineralocorticoid_receptor_antagonist: float, monoacylglycerol_lipase_inhibitor: float, monoamine_oxidase_inhibitor: float, monopolar_spindle_1_kinase_inhibitor: float, mtor_inhibitor: float, mucolytic_agent: float, neuropeptide_receptor_antagonist: float, nfkb_inhibitor: float, nicotinic_receptor_agonist: float, nitric_oxide_donor: float, nitric_oxide_production_inhibitor: float, nitric_oxide_synthase_inhibitor: float, norepinephrine_reuptake_inhibitor: float, nrf2_activator: float, opioid_receptor_agonist: float, opioid_receptor_antagonist: float, orexin_receptor_antagonist: float, p38_mapk_inhibitor: float, p-glycoprotein_inhibitor: float, parp_inhibitor: float, pdgfr_inhibitor: float, pdk_inhibitor: float, phosphodiesterase_inhibitor: float, phospholipase_inhibitor: float, pi3k_inhibitor: float, pkc_inhibitor: float, potassium_channel_activator: float, potassium_channel_antagonist: float, ppar_receptor_agonist: float, ppar_receptor_antagonist: float, progesterone_receptor_agonist: float, progesterone_receptor_antagonist: float, prostaglandin_inhibitor: float, prostanoid_receptor_antagonist: float, proteasome_inhibitor: float, protein_kinase_inhibitor: float, protein_phosphatase_inhibitor: float, protein_synthesis_inhibitor: float, protein_tyrosine_kinase_inhibitor: float, radiopaque_medium: float, raf_inhibitor: float, ras_gtpase_inhibitor: float, retinoid_receptor_agonist: float, retinoid_receptor_antagonist: float, rho_associated_kinase_inhibitor: float, ribonucleoside_reductase_inhibitor: float, rna_polymerase_inhibitor: float, serotonin_receptor_agonist: float, serotonin_receptor_antagonist: float, serotonin_reuptake_inhibitor: float, sigma_receptor_agonist: float, sigma_receptor_antagonist: float, smoothened_receptor_antagonist: float, sodium_channel_inhibitor: float, sphingosine_receptor_agonist: float, src_inhibitor: float, steroid: float, syk_inhibitor: float, tachykinin_antagonist: float, tgf-beta_receptor_inhibitor: float, thrombin_inhibitor: float, thymidylate_synthase_inhibitor: float, tlr_agonist: float, tlr_antagonist: float, tnf_inhibitor: float, topoisomerase_inhibitor: float, transient_receptor_potential_channel_antagonist: float, tropomyosin_receptor_kinase_inhibitor: float, trpv_agonist: float, trpv_antagonist: float, tubulin_inhibitor: float, tyrosine_kinase_inhibitor: float, ubiquitin_specific_protease_inhibitor: float, vegfr_inhibitor: float, vitamin_b: float, vitamin_d_receptor_agonist: float, wnt_inhibitor: float]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dt.fit(train).transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation score after tuning for Decision Tree model is : \n",
      "+--------------------+\n",
      "|               score|\n",
      "+--------------------+\n",
      "|0.020293238203227204|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'The validation score after tuning for Decision Tree model is : ')\n",
    "final_dt.score().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [04:29,  1.31s/it]\n",
      "206it [01:34,  2.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, 5-alpha_reductase_inhibitor: float, 11-beta-hsd1_inhibitor: float, acat_inhibitor: float, acetylcholine_receptor_agonist: float, acetylcholine_receptor_antagonist: float, acetylcholinesterase_inhibitor: float, adenosine_receptor_agonist: float, adenosine_receptor_antagonist: float, adenylyl_cyclase_activator: float, adrenergic_receptor_agonist: float, adrenergic_receptor_antagonist: float, akt_inhibitor: float, aldehyde_dehydrogenase_inhibitor: float, alk_inhibitor: float, ampk_activator: float, analgesic: float, androgen_receptor_agonist: float, androgen_receptor_antagonist: float, anesthetic_-_local: float, angiogenesis_inhibitor: float, angiotensin_receptor_antagonist: float, anti-inflammatory: float, antiarrhythmic: float, antibiotic: float, anticonvulsant: float, antifungal: float, antihistamine: float, antimalarial: float, antioxidant: float, antiprotozoal: float, antiviral: float, apoptosis_stimulant: float, aromatase_inhibitor: float, atm_kinase_inhibitor: float, atp-sensitive_potassium_channel_antagonist: float, atp_synthase_inhibitor: float, atpase_inhibitor: float, atr_kinase_inhibitor: float, aurora_kinase_inhibitor: float, autotaxin_inhibitor: float, bacterial_30s_ribosomal_subunit_inhibitor: float, bacterial_50s_ribosomal_subunit_inhibitor: float, bacterial_antifolate: float, bacterial_cell_wall_synthesis_inhibitor: float, bacterial_dna_gyrase_inhibitor: float, bacterial_dna_inhibitor: float, bacterial_membrane_integrity_inhibitor: float, bcl_inhibitor: float, bcr-abl_inhibitor: float, benzodiazepine_receptor_agonist: float, beta_amyloid_inhibitor: float, bromodomain_inhibitor: float, btk_inhibitor: float, calcineurin_inhibitor: float, calcium_channel_blocker: float, cannabinoid_receptor_agonist: float, cannabinoid_receptor_antagonist: float, carbonic_anhydrase_inhibitor: float, casein_kinase_inhibitor: float, caspase_activator: float, catechol_o_methyltransferase_inhibitor: float, cc_chemokine_receptor_antagonist: float, cck_receptor_antagonist: float, cdk_inhibitor: float, chelating_agent: float, chk_inhibitor: float, chloride_channel_blocker: float, cholesterol_inhibitor: float, cholinergic_receptor_antagonist: float, coagulation_factor_inhibitor: float, corticosteroid_agonist: float, cyclooxygenase_inhibitor: float, cytochrome_p450_inhibitor: float, dihydrofolate_reductase_inhibitor: float, dipeptidyl_peptidase_inhibitor: float, diuretic: float, dna_alkylating_agent: float, dna_inhibitor: float, dopamine_receptor_agonist: float, dopamine_receptor_antagonist: float, egfr_inhibitor: float, elastase_inhibitor: float, erbb2_inhibitor: float, estrogen_receptor_agonist: float, estrogen_receptor_antagonist: float, faah_inhibitor: float, farnesyltransferase_inhibitor: float, fatty_acid_receptor_agonist: float, fgfr_inhibitor: float, flt3_inhibitor: float, focal_adhesion_kinase_inhibitor: float, free_radical_scavenger: float, fungal_squalene_epoxidase_inhibitor: float, gaba_receptor_agonist: float, gaba_receptor_antagonist: float, gamma_secretase_inhibitor: float, glucocorticoid_receptor_agonist: float, glutamate_inhibitor: float, glutamate_receptor_agonist: float, glutamate_receptor_antagonist: float, gonadotropin_receptor_agonist: float, gsk_inhibitor: float, hcv_inhibitor: float, hdac_inhibitor: float, histamine_receptor_agonist: float, histamine_receptor_antagonist: float, histone_lysine_demethylase_inhibitor: float, histone_lysine_methyltransferase_inhibitor: float, hiv_inhibitor: float, hmgcr_inhibitor: float, hsp_inhibitor: float, igf-1_inhibitor: float, ikk_inhibitor: float, imidazoline_receptor_agonist: float, immunosuppressant: float, insulin_secretagogue: float, insulin_sensitizer: float, integrin_inhibitor: float, jak_inhibitor: float, kit_inhibitor: float, laxative: float, leukotriene_inhibitor: float, leukotriene_receptor_antagonist: float, lipase_inhibitor: float, lipoxygenase_inhibitor: float, lxr_agonist: float, mdm_inhibitor: float, mek_inhibitor: float, membrane_integrity_inhibitor: float, mineralocorticoid_receptor_antagonist: float, monoacylglycerol_lipase_inhibitor: float, monoamine_oxidase_inhibitor: float, monopolar_spindle_1_kinase_inhibitor: float, mtor_inhibitor: float, mucolytic_agent: float, neuropeptide_receptor_antagonist: float, nfkb_inhibitor: float, nicotinic_receptor_agonist: float, nitric_oxide_donor: float, nitric_oxide_production_inhibitor: float, nitric_oxide_synthase_inhibitor: float, norepinephrine_reuptake_inhibitor: float, nrf2_activator: float, opioid_receptor_agonist: float, opioid_receptor_antagonist: float, orexin_receptor_antagonist: float, p38_mapk_inhibitor: float, p-glycoprotein_inhibitor: float, parp_inhibitor: float, pdgfr_inhibitor: float, pdk_inhibitor: float, phosphodiesterase_inhibitor: float, phospholipase_inhibitor: float, pi3k_inhibitor: float, pkc_inhibitor: float, potassium_channel_activator: float, potassium_channel_antagonist: float, ppar_receptor_agonist: float, ppar_receptor_antagonist: float, progesterone_receptor_agonist: float, progesterone_receptor_antagonist: float, prostaglandin_inhibitor: float, prostanoid_receptor_antagonist: float, proteasome_inhibitor: float, protein_kinase_inhibitor: float, protein_phosphatase_inhibitor: float, protein_synthesis_inhibitor: float, protein_tyrosine_kinase_inhibitor: float, radiopaque_medium: float, raf_inhibitor: float, ras_gtpase_inhibitor: float, retinoid_receptor_agonist: float, retinoid_receptor_antagonist: float, rho_associated_kinase_inhibitor: float, ribonucleoside_reductase_inhibitor: float, rna_polymerase_inhibitor: float, serotonin_receptor_agonist: float, serotonin_receptor_antagonist: float, serotonin_reuptake_inhibitor: float, sigma_receptor_agonist: float, sigma_receptor_antagonist: float, smoothened_receptor_antagonist: float, sodium_channel_inhibitor: float, sphingosine_receptor_agonist: float, src_inhibitor: float, steroid: float, syk_inhibitor: float, tachykinin_antagonist: float, tgf-beta_receptor_inhibitor: float, thrombin_inhibitor: float, thymidylate_synthase_inhibitor: float, tlr_agonist: float, tlr_antagonist: float, tnf_inhibitor: float, topoisomerase_inhibitor: float, transient_receptor_potential_channel_antagonist: float, tropomyosin_receptor_kinase_inhibitor: float, trpv_agonist: float, trpv_antagonist: float, tubulin_inhibitor: float, tyrosine_kinase_inhibitor: float, ubiquitin_specific_protease_inhibitor: float, vegfr_inhibitor: float, vitamin_b: float, vitamin_d_receptor_agonist: float, wnt_inhibitor: float]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## eval on test set\n",
    "test_dt =  MultiLabelClassifier(clf, labels, 'features', hyperparameters=hyperparameters, method=method)\n",
    "test_dt.fit(cv_train).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test score after tuning for Decision Tree model is : \n",
      "+--------------------+\n",
      "|               score|\n",
      "+--------------------+\n",
      "|0.020260023106331515|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'The test score after tuning for Decision Tree model is : ')\n",
    "test_dt.score().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
