{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.3 (default, Jul  2 2020 17:30:36)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "import findspark\n",
    "import re\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:\\\\Users\\\\msi\\\\Desktop\\\\spark\\\\spark-3.0.1-bin-hadoop3.2'\n",
    "findspark.init()\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, VectorSlicer, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, lit, udf, mean as _mean\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_features.csv', header=True, inferSchema=True)\n",
    "target_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_targets_scored.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\test_features.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, cp_type: string, cp_time: int, cp_dose: string, g-0: double, g-1: double, g-2: double, g-3: double, g-4: double, g-5: double, g-6: double, g-7: double, g-8: double, g-9: double, g-10: double, g-11: double, g-12: double, g-13: double, g-14: double, g-15: double, g-16: double, g-17: double, g-18: double, g-19: double, g-20: double, g-21: double, g-22: double, g-23: double, g-24: double, g-25: double, g-26: double, g-27: double, g-28: double, g-29: double, g-30: double, g-31: double, g-32: double, g-33: double, g-34: double, g-35: double, g-36: double, g-37: double, g-38: double, g-39: double, g-40: double, g-41: double, g-42: double, g-43: double, g-44: double, g-45: double, g-46: double, g-47: double, g-48: double, g-49: double, g-50: double, g-51: double, g-52: double, g-53: double, g-54: double, g-55: double, g-56: double, g-57: double, g-58: double, g-59: double, g-60: double, g-61: double, g-62: double, g-63: double, g-64: double, g-65: double, g-66: double, g-67: double, g-68: double, g-69: double, g-70: double, g-71: double, g-72: double, g-73: double, g-74: double, g-75: double, g-76: double, g-77: double, g-78: double, g-79: double, g-80: double, g-81: double, g-82: double, g-83: double, g-84: double, g-85: double, g-86: double, g-87: double, g-88: double, g-89: double, g-90: double, g-91: double, g-92: double, g-93: double, g-94: double, g-95: double, g-96: double, g-97: double, g-98: double, g-99: double, g-100: double, g-101: double, g-102: double, g-103: double, g-104: double, g-105: double, g-106: double, g-107: double, g-108: double, g-109: double, g-110: double, g-111: double, g-112: double, g-113: double, g-114: double, g-115: double, g-116: double, g-117: double, g-118: double, g-119: double, g-120: double, g-121: double, g-122: double, g-123: double, g-124: double, g-125: double, g-126: double, g-127: double, g-128: double, g-129: double, g-130: double, g-131: double, g-132: double, g-133: double, g-134: double, g-135: double, g-136: double, g-137: double, g-138: double, g-139: double, g-140: double, g-141: double, g-142: double, g-143: double, g-144: double, g-145: double, g-146: double, g-147: double, g-148: double, g-149: double, g-150: double, g-151: double, g-152: double, g-153: double, g-154: double, g-155: double, g-156: double, g-157: double, g-158: double, g-159: double, g-160: double, g-161: double, g-162: double, g-163: double, g-164: double, g-165: double, g-166: double, g-167: double, g-168: double, g-169: double, g-170: double, g-171: double, g-172: double, g-173: double, g-174: double, g-175: double, g-176: double, g-177: double, g-178: double, g-179: double, g-180: double, g-181: double, g-182: double, g-183: double, g-184: double, g-185: double, g-186: double, g-187: double, g-188: double, g-189: double, g-190: double, g-191: double, g-192: double, g-193: double, g-194: double, g-195: double, g-196: double, g-197: double, g-198: double, g-199: double, g-200: double, g-201: double, g-202: double, g-203: double, g-204: double, g-205: double, g-206: double, g-207: double, g-208: double, g-209: double, g-210: double, g-211: double, g-212: double, g-213: double, g-214: double, g-215: double, g-216: double, g-217: double, g-218: double, g-219: double, g-220: double, g-221: double, g-222: double, g-223: double, g-224: double, g-225: double, g-226: double, g-227: double, g-228: double, g-229: double, g-230: double, g-231: double, g-232: double, g-233: double, g-234: double, g-235: double, g-236: double, g-237: double, g-238: double, g-239: double, g-240: double, g-241: double, g-242: double, g-243: double, g-244: double, g-245: double, g-246: double, g-247: double, g-248: double, g-249: double, g-250: double, g-251: double, g-252: double, g-253: double, g-254: double, g-255: double, g-256: double, g-257: double, g-258: double, g-259: double, g-260: double, g-261: double, g-262: double, g-263: double, g-264: double, g-265: double, g-266: double, g-267: double, g-268: double, g-269: double, g-270: double, g-271: double, g-272: double, g-273: double, g-274: double, g-275: double, g-276: double, g-277: double, g-278: double, g-279: double, g-280: double, g-281: double, g-282: double, g-283: double, g-284: double, g-285: double, g-286: double, g-287: double, g-288: double, g-289: double, g-290: double, g-291: double, g-292: double, g-293: double, g-294: double, g-295: double, g-296: double, g-297: double, g-298: double, g-299: double, g-300: double, g-301: double, g-302: double, g-303: double, g-304: double, g-305: double, g-306: double, g-307: double, g-308: double, g-309: double, g-310: double, g-311: double, g-312: double, g-313: double, g-314: double, g-315: double, g-316: double, g-317: double, g-318: double, g-319: double, g-320: double, g-321: double, g-322: double, g-323: double, g-324: double, g-325: double, g-326: double, g-327: double, g-328: double, g-329: double, g-330: double, g-331: double, g-332: double, g-333: double, g-334: double, g-335: double, g-336: double, g-337: double, g-338: double, g-339: double, g-340: double, g-341: double, g-342: double, g-343: double, g-344: double, g-345: double, g-346: double, g-347: double, g-348: double, g-349: double, g-350: double, g-351: double, g-352: double, g-353: double, g-354: double, g-355: double, g-356: double, g-357: double, g-358: double, g-359: double, g-360: double, g-361: double, g-362: double, g-363: double, g-364: double, g-365: double, g-366: double, g-367: double, g-368: double, g-369: double, g-370: double, g-371: double, g-372: double, g-373: double, g-374: double, g-375: double, g-376: double, g-377: double, g-378: double, g-379: double, g-380: double, g-381: double, g-382: double, g-383: double, g-384: double, g-385: double, g-386: double, g-387: double, g-388: double, g-389: double, g-390: double, g-391: double, g-392: double, g-393: double, g-394: double, g-395: double, g-396: double, g-397: double, g-398: double, g-399: double, g-400: double, g-401: double, g-402: double, g-403: double, g-404: double, g-405: double, g-406: double, g-407: double, g-408: double, g-409: double, g-410: double, g-411: double, g-412: double, g-413: double, g-414: double, g-415: double, g-416: double, g-417: double, g-418: double, g-419: double, g-420: double, g-421: double, g-422: double, g-423: double, g-424: double, g-425: double, g-426: double, g-427: double, g-428: double, g-429: double, g-430: double, g-431: double, g-432: double, g-433: double, g-434: double, g-435: double, g-436: double, g-437: double, g-438: double, g-439: double, g-440: double, g-441: double, g-442: double, g-443: double, g-444: double, g-445: double, g-446: double, g-447: double, g-448: double, g-449: double, g-450: double, g-451: double, g-452: double, g-453: double, g-454: double, g-455: double, g-456: double, g-457: double, g-458: double, g-459: double, g-460: double, g-461: double, g-462: double, g-463: double, g-464: double, g-465: double, g-466: double, g-467: double, g-468: double, g-469: double, g-470: double, g-471: double, g-472: double, g-473: double, g-474: double, g-475: double, g-476: double, g-477: double, g-478: double, g-479: double, g-480: double, g-481: double, g-482: double, g-483: double, g-484: double, g-485: double, g-486: double, g-487: double, g-488: double, g-489: double, g-490: double, g-491: double, g-492: double, g-493: double, g-494: double, g-495: double, g-496: double, g-497: double, g-498: double, g-499: double, g-500: double, g-501: double, g-502: double, g-503: double, g-504: double, g-505: double, g-506: double, g-507: double, g-508: double, g-509: double, g-510: double, g-511: double, g-512: double, g-513: double, g-514: double, g-515: double, g-516: double, g-517: double, g-518: double, g-519: double, g-520: double, g-521: double, g-522: double, g-523: double, g-524: double, g-525: double, g-526: double, g-527: double, g-528: double, g-529: double, g-530: double, g-531: double, g-532: double, g-533: double, g-534: double, g-535: double, g-536: double, g-537: double, g-538: double, g-539: double, g-540: double, g-541: double, g-542: double, g-543: double, g-544: double, g-545: double, g-546: double, g-547: double, g-548: double, g-549: double, g-550: double, g-551: double, g-552: double, g-553: double, g-554: double, g-555: double, g-556: double, g-557: double, g-558: double, g-559: double, g-560: double, g-561: double, g-562: double, g-563: double, g-564: double, g-565: double, g-566: double, g-567: double, g-568: double, g-569: double, g-570: double, g-571: double, g-572: double, g-573: double, g-574: double, g-575: double, g-576: double, g-577: double, g-578: double, g-579: double, g-580: double, g-581: double, g-582: double, g-583: double, g-584: double, g-585: double, g-586: double, g-587: double, g-588: double, g-589: double, g-590: double, g-591: double, g-592: double, g-593: double, g-594: double, g-595: double, g-596: double, g-597: double, g-598: double, g-599: double, g-600: double, g-601: double, g-602: double, g-603: double, g-604: double, g-605: double, g-606: double, g-607: double, g-608: double, g-609: double, g-610: double, g-611: double, g-612: double, g-613: double, g-614: double, g-615: double, g-616: double, g-617: double, g-618: double, g-619: double, g-620: double, g-621: double, g-622: double, g-623: double, g-624: double, g-625: double, g-626: double, g-627: double, g-628: double, g-629: double, g-630: double, g-631: double, g-632: double, g-633: double, g-634: double, g-635: double, g-636: double, g-637: double, g-638: double, g-639: double, g-640: double, g-641: double, g-642: double, g-643: double, g-644: double, g-645: double, g-646: double, g-647: double, g-648: double, g-649: double, g-650: double, g-651: double, g-652: double, g-653: double, g-654: double, g-655: double, g-656: double, g-657: double, g-658: double, g-659: double, g-660: double, g-661: double, g-662: double, g-663: double, g-664: double, g-665: double, g-666: double, g-667: double, g-668: double, g-669: double, g-670: double, g-671: double, g-672: double, g-673: double, g-674: double, g-675: double, g-676: double, g-677: double, g-678: double, g-679: double, g-680: double, g-681: double, g-682: double, g-683: double, g-684: double, g-685: double, g-686: double, g-687: double, g-688: double, g-689: double, g-690: double, g-691: double, g-692: double, g-693: double, g-694: double, g-695: double, g-696: double, g-697: double, g-698: double, g-699: double, g-700: double, g-701: double, g-702: double, g-703: double, g-704: double, g-705: double, g-706: double, g-707: double, g-708: double, g-709: double, g-710: double, g-711: double, g-712: double, g-713: double, g-714: double, g-715: double, g-716: double, g-717: double, g-718: double, g-719: double, g-720: double, g-721: double, g-722: double, g-723: double, g-724: double, g-725: double, g-726: double, g-727: double, g-728: double, g-729: double, g-730: double, g-731: double, g-732: double, g-733: double, g-734: double, g-735: double, g-736: double, g-737: double, g-738: double, g-739: double, g-740: double, g-741: double, g-742: double, g-743: double, g-744: double, g-745: double, g-746: double, g-747: double, g-748: double, g-749: double, g-750: double, g-751: double, g-752: double, g-753: double, g-754: double, g-755: double, g-756: double, g-757: double, g-758: double, g-759: double, g-760: double, g-761: double, g-762: double, g-763: double, g-764: double, g-765: double, g-766: double, g-767: double, g-768: double, g-769: double, g-770: double, g-771: double, c-0: double, c-1: double, c-2: double, c-3: double, c-4: double, c-5: double, c-6: double, c-7: double, c-8: double, c-9: double, c-10: double, c-11: double, c-12: double, c-13: double, c-14: double, c-15: double, c-16: double, c-17: double, c-18: double, c-19: double, c-20: double, c-21: double, c-22: double, c-23: double, c-24: double, c-25: double, c-26: double, c-27: double, c-28: double, c-29: double, c-30: double, c-31: double, c-32: double, c-33: double, c-34: double, c-35: double, c-36: double, c-37: double, c-38: double, c-39: double, c-40: double, c-41: double, c-42: double, c-43: double, c-44: double, c-45: double, c-46: double, c-47: double, c-48: double, c-49: double, c-50: double, c-51: double, c-52: double, c-53: double, c-54: double, c-55: double, c-56: double, c-57: double, c-58: double, c-59: double, c-60: double, c-61: double, c-62: double, c-63: double, c-64: double, c-65: double, c-66: double, c-67: double, c-68: double, c-69: double, c-70: double, c-71: double, c-72: double, c-73: double, c-74: double, c-75: double, c-76: double, c-77: double, c-78: double, c-79: double, c-80: double, c-81: double, c-82: double, c-83: double, c-84: double, c-85: double, c-86: double, c-87: double, c-88: double, c-89: double, c-90: double, c-91: double, c-92: double, c-93: double, c-94: double, c-95: double, c-96: double, c-97: double, c-98: double, c-99: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "target_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_features(df, cat_features):\n",
    "\n",
    "  indexed_cols = [''.join([col_name, '_indexed']) for col_name in cat_features]\n",
    "  encoded_cols = [''.join([col_name, '_encoded']) for col_name in cat_features]\n",
    "  string_indexers = [StringIndexer(inputCol=cat_features[i], outputCol=indexed_cols[i]) for i in range(len(cat_features))]\n",
    "    \n",
    "  encoder = OneHotEncoder(inputCols=indexed_cols, outputCols=encoded_cols)\n",
    "  \n",
    "  pipline = Pipeline(stages=string_indexers + [encoder])\n",
    "  \n",
    "  encoded_df = pipline.fit(df).transform(df)\n",
    "  encoded_df = encoded_df.drop(*indexed_cols + cat_features)\n",
    "\n",
    "  return encoded_df\n",
    "\n",
    "def normalize_features(df, cols, normalizer, output_cols, if_drop=True):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  normalizer_lst = []\n",
    "  vectorized_cols = []\n",
    "  vector_assembers = []\n",
    "  \n",
    "  if isinstance(cols, list):\n",
    "    cols = {'cols': cols}\n",
    "  \n",
    "  if isinstance(output_cols, str):\n",
    "    output_cols = {'cols': output_cols}\n",
    "  \n",
    "  for k, v in cols.items():\n",
    "    \n",
    "    temp_normalizer = normalizer.copy()\n",
    "    vectorized_col = ''.join([output_cols[k], '_v'])\n",
    "    vector_assember = VectorAssembler(inputCols=v, outputCol=vectorized_col)\n",
    "    \n",
    "    temp_normalizer.setInputCol(vectorized_col)\n",
    "    temp_normalizer.setOutputCol(output_cols[k])\n",
    "    \n",
    "    normalizer_lst.append(temp_normalizer)\n",
    "    vectorized_cols.append(vectorized_col)\n",
    "    vector_assembers.append(vector_assember)\n",
    "  \n",
    "  pipline = Pipeline(stages=vector_assembers + normalizer_lst)\n",
    "  normalized_df = pipline.fit(df).transform(df).drop(*vectorized_cols)\n",
    "  \n",
    "  if if_drop:\n",
    "    \n",
    "    for k, v in cols.items():\n",
    "      \n",
    "      normalized_df = normalized_df.drop(v)\n",
    "  \n",
    "  return normalized_df\n",
    "\n",
    "def add_pca_features(df, g_cols, c_cols, k=40):\n",
    "  \n",
    "  ## normalize g-col and c-col\n",
    "  std_scaler = StandardScaler(withMean=True)\n",
    "  \n",
    "  input_cols = {\n",
    "    'g_cols': g_cols, \n",
    "    'c_cols': c_cols}\n",
    "  \n",
    "  output_cols = {\n",
    "    'g_cols': 'g_normalized', \n",
    "    'c_cols': 'c_normalized'}\n",
    "  \n",
    "  normalized_df = normalize_features(df, input_cols, std_scaler, output_cols, if_drop=False)\n",
    "  \n",
    "  ## perform PCA on g-cols and c-cols\n",
    "  g_col_pca = PCA(k=k, inputCol='g_normalized', outputCol='g_col_pca')\n",
    "  c_col_pca = PCA(k=k, inputCol='c_normalized', outputCol='c_col_pca')\n",
    "  \n",
    "  pipeline = Pipeline(stages=[g_col_pca, c_col_pca])\n",
    "  pca_df = pipeline.fit(normalized_df).transform(normalized_df)\n",
    "  \n",
    "  return pca_df\n",
    "  \n",
    "def add_stats_features(df, g_cols, c_cols):\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_sum(*lst):\n",
    "\n",
    "    return sum(lst)\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_mean(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst)\n",
    "\n",
    "    return s / n\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_var(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst) / n\n",
    "    total = 0\n",
    "\n",
    "    for x in lst:\n",
    "\n",
    "      total += (x - s)**2 \n",
    "\n",
    "    return total / n\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_min(*lst):\n",
    "    \n",
    "    return min(lst)\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_max(*lst):\n",
    "    \n",
    "    return max(lst)\n",
    "  \n",
    "  stats_dict = {\n",
    "    'min_stats': cols_min,\n",
    "    'max_stats': cols_max,\n",
    "    'var_stats': cols_var,\n",
    "    'mean_stats': cols_mean,\n",
    "    'sum_stats': cols_sum\n",
    "  }\n",
    "  \n",
    "  for name, func in stats_dict.items():\n",
    "    \n",
    "    df = df.withColumn(''.join(['g_cols_', name]), func(*[col(g_col) for g_col in g_cols]))\n",
    "    df = df.withColumn(''.join(['c_cols_', name]), func(*[col(c_col) for c_col in c_cols]))\n",
    "  \n",
    "  return df\n",
    "\n",
    "def add_kmeans_features(df, g_cols, c_cols, k=2, num_iter=10):\n",
    "  \n",
    "  kmeans_g = KMeans(k=k, featuresCol=g_cols, predictionCol='g_col_k_mean', seed=16)\n",
    "  kmeans_c = KMeans(k=k, featuresCol=c_cols, predictionCol='c_col_k_mean', seed=16)\n",
    "  \n",
    "  kmeans_df = kmeans_g.fit(df).transform(df)\n",
    "  kmeans_df = kmeans_c.fit(kmeans_df).transform(kmeans_df)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def feature_engineering(df, num_cluster=2, num_comp=40, num_iter=10):\n",
    "  \n",
    "  ## get g-col and c-col\n",
    "  g_cols = list(filter(lambda v: re.match('g-.+', v), df.columns))\n",
    "  c_cols = list(filter(lambda v: re.match('c-.+', v), df.columns))\n",
    "  \n",
    "  ## PCA\n",
    "  pca_df = add_pca_features(df, g_cols, c_cols, num_comp)\n",
    "\n",
    "  ## stats features on g and c cols\n",
    "  stats_df = add_stats_features(pca_df, g_cols, c_cols)\n",
    "  \n",
    "  ## add k-means features\n",
    "  kmeans_df = add_kmeans_features(stats_df, g_cols='g_normalized', c_cols='c_normalized', k=num_cluster, num_iter=num_iter)\n",
    "  \n",
    "  return kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add indicator column to both train and test so we can combine them later\n",
    "train_df = train_df.withColumn('is_test', lit(0))\n",
    "test_df = test_df.withColumn('is_test', lit(1))\n",
    "\n",
    "## Combine train and test df\n",
    "full_df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## encode features\n",
    "target_cols = ['cp_type', 'cp_dose']\n",
    "encoded_df = encode_cat_features(full_df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering\n",
    "fe_df = feature_engineering(encoded_df, num_comp=20, num_iter=5)\n",
    "\n",
    "## select all the feature columns\n",
    "\n",
    "pca_cols = list(filter(lambda v: re.match('.+_pca', v), fe_df.columns))\n",
    "stats_cols = list(filter(lambda v: re.match('.+_stats', v), fe_df.columns))\n",
    "k_means_cols = list(filter(lambda v: re.match('.+_k_mean', v), fe_df.columns))\n",
    "cat_cols = list(filter(lambda v: re.match('.+_encoded', v), fe_df.columns)) + ['cp_time']\n",
    "\n",
    "## stack them to a single feature vector\n",
    "vector_assember_train = VectorAssembler(inputCols=pca_cols + stats_cols + k_means_cols + cat_cols, outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df)\n",
    "\n",
    "## normalize all the features\n",
    "normalizer = StandardScaler(withMean=True)\n",
    "cols = ['all_features']\n",
    "output_cols = 'features'\n",
    "fe_df = normalize_features(fe_df, cols, normalizer, output_cols, if_drop=False)\n",
    "\n",
    "## split train, test df\n",
    "fe_train = fe_df.filter(fe_df['is_test'] == 0)\n",
    "final_test = fe_df.filter(fe_df['is_test'] == 1).select(['sig_id', 'features'])\n",
    "\n",
    "## stack the training target\n",
    "vector_assember_target = VectorAssembler(inputCols=[c for c in target_df.columns if c not in {'sig_id'}], outputCol='targets')\n",
    "vectorized_target = vector_assember_target.transform(target_df).select(['sig_id', 'targets'])\n",
    "\n",
    "## join training target with training features\n",
    "final_train = fe_train.join(vectorized_target, ['sig_id']).select(['sig_id', 'features', 'targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sig_id: string, features: vector, targets: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train test split\n",
    "(train, validation) = final_train.randomSplit([0.8, 0.2], 16)\n",
    "\n",
    "train.cache()\n",
    "validation.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleBaseClfs:\n",
    "\n",
    "    def __init__(self, base_clf, num_classes, feature_col, target_col, output_col='prediction'):\n",
    "        \n",
    "        self.base_clf = base_clf\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_col = feature_col\n",
    "        self.target_col = target_col\n",
    "        self.output_col = output_col\n",
    "        \n",
    "        self._predict_cols = []\n",
    "        self._trained_clfs = []\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        for c in tqdm(range(num_classes)):\n",
    "            \n",
    "            predict_col_name = f'prob_{c}'\n",
    "            temp_clf = self.base_clf.copy()\n",
    "            temp_train = train.withColumn('new_target', vector_to_array(self.target_col)[c])\n",
    "            \n",
    "            temp_clf.setLabelCol('new_target')\n",
    "            temp_clf.setFeaturesCol(self.feature_col)\n",
    "            \n",
    "            try:\n",
    "                temp_clf.setProbabilityCol(predict_col_name)\n",
    "            except e:\n",
    "                print('classifier cannot output probability')\n",
    "                temp_clf.setPredictionCol(predict_col_name)\n",
    "                \n",
    "            temp_clf = temp_clf.fit(temp_train)\n",
    "            self._trained_clfs.append(temp_clf)\n",
    "            self._predict_cols.append(predict_col_name)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test, to_pandas=False):\n",
    "        \n",
    "        curr_df = x_test\n",
    "        input_cols = x_test.columns\n",
    "        \n",
    "        for i, clf in tqdm(enumerate(self._trained_clfs)):\n",
    "            \n",
    "            ## select current probability column prob_i\n",
    "            prob_col = self._predict_cols[i]\n",
    "            input_cols.append(prob_col)\n",
    "            curr_df = clf.transform(curr_df)\n",
    "            curr_df = curr_df.withColumn(prob_col, vector_to_array(prob_col)[1])\n",
    "            curr_df = curr_df.select(input_cols)\n",
    "        \n",
    "        ## transform columns prob_0, ..., prob_c to a vector with name self.output_col\n",
    "        \n",
    "        if to_pandas:\n",
    "            \n",
    "            return curr_df.toPandas()\n",
    "        \n",
    "        else:\n",
    "            va = VectorAssembler(inputCols=self._predict_cols, outputCol=self.output_col)\n",
    "            ## transform this vector self.output_col to an array\n",
    "            output_df = va.transform(curr_df).withColumn(self.output_col, vector_to_array(self.output_col)).select(x_test.columns + [self.output_col])\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    \n",
    "    def evaluate(self, output_df):\n",
    "        \n",
    "        ## transform target vector to array\n",
    "        output_df = output_df.withColumn(self.target_col, vector_to_array(self.target_col))\n",
    "        \n",
    "        def score(df, predictCol, targetsCol):\n",
    "            import math\n",
    "            @udf('double')\n",
    "            def log_loss(y, y_hat):\n",
    "                r = 0\n",
    "                cut = 10**(-15)\n",
    "                for t, p in zip(y, y_hat):\n",
    "                    p = max(min(p, 1-cut),cut)\n",
    "                    r += t * math.log(p) + (1 - t) * math.log(1 - p)\n",
    "                return r/len(y)\n",
    "            log_loss = df.select(log_loss(targetsCol, predictCol).alias('log_loss'))\n",
    "            return log_loss.select((-_mean(col('log_loss'))).alias('score'))\n",
    "        \n",
    "        ## calcualte score\n",
    "        return score(output_df, self.output_col, self.target_col)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 206/206 [07:39<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "base_clf = LogisticRegression(maxIter=2)\n",
    "num_classes = 206\n",
    "feature_col = 'features'\n",
    "target_col = 'targets'\n",
    "clf = EnsembleBaseClfs(base_clf, num_classes, feature_col, target_col).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [01:25,  2.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[score: double]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = clf.predict(validation)\n",
    "clf.evaluate(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              score|\n",
      "+-------------------+\n",
      "|0.01776026006612206|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.evaluate(output_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
