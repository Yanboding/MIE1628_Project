{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os    \n",
    "import findspark\n",
    "import re\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:\\\\Users\\\\msi\\\\Desktop\\\\spark\\\\spark-3.0.1-bin-hadoop3.2'\n",
    "findspark.init()\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, VectorSlicer, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes, DecisionTreeClassifier\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, lit, udf, mean as _mean, isnan, sum as _sum, log as _log\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import FloatType, ArrayType, IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_features.csv', header=True, inferSchema=True)\n",
    "target_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\train_targets_scored.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('C:\\\\Users\\\\msi\\\\Onedrive\\\\MOA\\\\test_features.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.cache()\n",
    "target_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_features(df, cat_features):\n",
    "\n",
    "  indexed_cols = [''.join([col_name, '_indexed']) for col_name in cat_features]\n",
    "  encoded_cols = [''.join([col_name, '_encoded']) for col_name in cat_features]\n",
    "  string_indexers = [StringIndexer(inputCol=cat_features[i], outputCol=indexed_cols[i]) for i in range(len(cat_features))]\n",
    "    \n",
    "  encoder = OneHotEncoder(inputCols=indexed_cols, outputCols=encoded_cols)\n",
    "  \n",
    "  pipline = Pipeline(stages=string_indexers + [encoder])\n",
    "  \n",
    "  encoded_df = pipline.fit(df).transform(df)\n",
    "  encoded_df = encoded_df.drop(*indexed_cols + cat_features)\n",
    "\n",
    "  return encoded_df\n",
    "\n",
    "def normalize_features(df, cols, normalizer, output_cols, if_drop=True):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  normalizer_lst = []\n",
    "  vectorized_cols = []\n",
    "  vector_assembers = []\n",
    "  \n",
    "  if isinstance(cols, list):\n",
    "    cols = {'cols': cols}\n",
    "  \n",
    "  if isinstance(output_cols, str):\n",
    "    output_cols = {'cols': output_cols}\n",
    "  \n",
    "  for k, v in cols.items():\n",
    "    \n",
    "    temp_normalizer = normalizer.copy()\n",
    "    vectorized_col = ''.join([output_cols[k], '_v'])\n",
    "    vector_assember = VectorAssembler(inputCols=v, outputCol=vectorized_col)\n",
    "    \n",
    "    temp_normalizer.setInputCol(vectorized_col)\n",
    "    temp_normalizer.setOutputCol(output_cols[k])\n",
    "    \n",
    "    normalizer_lst.append(temp_normalizer)\n",
    "    vectorized_cols.append(vectorized_col)\n",
    "    vector_assembers.append(vector_assember)\n",
    "  \n",
    "  pipline = Pipeline(stages=vector_assembers + normalizer_lst)\n",
    "  normalized_df = pipline.fit(df).transform(df).drop(*vectorized_cols)\n",
    "  \n",
    "  if if_drop:\n",
    "    \n",
    "    for k, v in cols.items():\n",
    "      \n",
    "      normalized_df = normalized_df.drop(v)\n",
    "  \n",
    "  return normalized_df\n",
    "\n",
    "def add_pca_features(df, g_cols, c_cols, k=40):\n",
    "  \n",
    "  ## normalize g-col and c-col\n",
    "  std_scaler = StandardScaler(withMean=True)\n",
    "  \n",
    "  input_cols = {\n",
    "    'g_cols': g_cols, \n",
    "    'c_cols': c_cols}\n",
    "  \n",
    "  output_cols = {\n",
    "    'g_cols': 'g_normalized', \n",
    "    'c_cols': 'c_normalized'}\n",
    "  \n",
    "  normalized_df = normalize_features(df, input_cols, std_scaler, output_cols, if_drop=False)\n",
    "  \n",
    "  ## perform PCA on g-cols and c-cols\n",
    "  g_col_pca = PCA(k=k, inputCol='g_normalized', outputCol='g_col_pca')\n",
    "  c_col_pca = PCA(k=k, inputCol='c_normalized', outputCol='c_col_pca')\n",
    "  \n",
    "  pipeline = Pipeline(stages=[g_col_pca, c_col_pca])\n",
    "  pca_df = pipeline.fit(normalized_df).transform(normalized_df)\n",
    "  \n",
    "  return pca_df\n",
    "  \n",
    "def add_stats_features(df, g_cols, c_cols):\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_sum(*lst):\n",
    "\n",
    "    return sum(lst)\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_mean(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst)\n",
    "\n",
    "    return s / n\n",
    "\n",
    "  @udf('double')\n",
    "  def cols_var(*lst):\n",
    "\n",
    "    n = len(lst)\n",
    "    s = sum(lst) / n\n",
    "    total = 0\n",
    "\n",
    "    for x in lst:\n",
    "\n",
    "      total += (x - s)**2 \n",
    "\n",
    "    return total / n\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_min(*lst):\n",
    "    \n",
    "    return min(lst)\n",
    "  \n",
    "  @udf('double')\n",
    "  def cols_max(*lst):\n",
    "    \n",
    "    return max(lst)\n",
    "  \n",
    "  stats_dict = {\n",
    "    'min_stats': cols_min,\n",
    "    'max_stats': cols_max,\n",
    "    'var_stats': cols_var,\n",
    "    'mean_stats': cols_mean,\n",
    "    'sum_stats': cols_sum\n",
    "  }\n",
    "  \n",
    "  for name, func in stats_dict.items():\n",
    "    \n",
    "    df = df.withColumn(''.join(['g_cols_', name]), func(*[col(g_col) for g_col in g_cols]))\n",
    "    df = df.withColumn(''.join(['c_cols_', name]), func(*[col(c_col) for c_col in c_cols]))\n",
    "  \n",
    "  return df\n",
    "\n",
    "def add_kmeans_features(df, g_cols, c_cols, k=2, num_iter=10):\n",
    "  \n",
    "  kmeans_g = KMeans(k=k, featuresCol=g_cols, predictionCol='g_col_k_mean', seed=16)\n",
    "  kmeans_c = KMeans(k=k, featuresCol=c_cols, predictionCol='c_col_k_mean', seed=16)\n",
    "  \n",
    "  kmeans_df = kmeans_g.fit(df).transform(df)\n",
    "  kmeans_df = kmeans_c.fit(kmeans_df).transform(kmeans_df)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def feature_engineering(df, num_cluster=2, num_comp=40, num_iter=10):\n",
    "  \n",
    "  ## get g-col and c-col\n",
    "  g_cols = list(filter(lambda v: re.match('g-.+', v), df.columns))\n",
    "  c_cols = list(filter(lambda v: re.match('c-.+', v), df.columns))\n",
    "  \n",
    "  ## PCA\n",
    "  pca_df = add_pca_features(df, g_cols, c_cols, num_comp)\n",
    "\n",
    "  ## stats features on g and c cols\n",
    "  stats_df = add_stats_features(pca_df, g_cols, c_cols)\n",
    "  \n",
    "  ## add k-means features\n",
    "  kmeans_df = add_kmeans_features(stats_df, g_cols='g_normalized', c_cols='c_normalized', k=num_cluster, num_iter=num_iter)\n",
    "  \n",
    "  return kmeans_df\n",
    "\n",
    "def get_correlation(df, threshold=0.95, feature_col='all_features', plot=False):\n",
    "    \n",
    "    from pyspark.ml.stat import Correlation\n",
    "    from pyspark.sql.types import FloatType, ArrayType, IntegerType\n",
    "    import numpy as np\n",
    "    \n",
    "    r1 = Correlation.corr(df, feature_col)\n",
    "    r1 = r1.select(f'pearson({feature_col})').collect()[0][f'pearson({feature_col})'].toArray().tolist()\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        cor_plt_array = np.array(r1)\n",
    "        sns.heatmap(cor_plt_array)\n",
    "        \n",
    "    r1 = spark.createDataFrame(r1)\n",
    "    gv = sc.broadcast(threshold)\n",
    "    va = VectorAssembler(inputCols=r1.columns, outputCol='features')\n",
    "    r1 = va.transform(r1)\n",
    "    \n",
    "    def find_element(row, gv):\n",
    "    \n",
    "        lst = []\n",
    "\n",
    "        for i in range(len(row)):\n",
    "\n",
    "            if (row[i] >= gv.value) | (row[i] <= -gv.value):\n",
    "                lst.append(i)\n",
    "        \n",
    "        if len(lst) == 1:\n",
    "            lst = []\n",
    "            \n",
    "        return lst\n",
    "\n",
    "    m = udf(lambda x: find_element(x, gv), ArrayType(IntegerType()))\n",
    "    \n",
    "    return r1.withColumn('new_col', m('features')).select('new_col')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add indicator column to both train and test so we can combine them later\n",
    "train_df = train_df.withColumn('is_test', lit(0))\n",
    "test_df = test_df.withColumn('is_test', lit(1))\n",
    "\n",
    "## Combine train and test df\n",
    "full_df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode features\n",
    "target_cols = ['cp_type', 'cp_dose']\n",
    "encoded_df = encode_cat_features(full_df, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## feature engineering\n",
    "fe_df = feature_engineering(encoded_df, num_comp=20, num_iter=5)\n",
    "\n",
    "## select all the feature columns\n",
    "\n",
    "pca_cols = list(filter(lambda v: re.match('.+_pca', v), fe_df.columns))\n",
    "stats_cols = list(filter(lambda v: re.match('.+_stats', v), fe_df.columns))\n",
    "k_means_cols = list(filter(lambda v: re.match('.+_k_mean', v), fe_df.columns))\n",
    "cat_cols = list(filter(lambda v: re.match('.+_encoded', v), fe_df.columns)) + ['cp_time']\n",
    "all_cols = pca_cols + stats_cols + k_means_cols + cat_cols\n",
    "## stack them to a single feature vector\n",
    "vector_assember_train = VectorAssembler(inputCols=all_cols, outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df)\n",
    "\n",
    "## Check their correlation\n",
    "corr_df = get_correlation(fe_df, 0.96, plot=True)\n",
    "corr_df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 40 PCA columns\n",
    "## looking for position 3, 6, 7, 8\n",
    "(stats_cols + k_means_cols + cat_cols)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats_cols + k_means_cols + cat_cols)[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features = ['c_cols_max_stats', 'g_cols_mean_stats', 'c_cols_mean_stats', 'g_cols_sum_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stack them to a single feature vector again, and remove those features with high correlation\n",
    "vector_assember_train = VectorAssembler(inputCols=[col for col in all_cols if col not in high_corr_features], outputCol='all_features')\n",
    "fe_df = vector_assember_train.transform(fe_df.drop('all_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize all the features\n",
    "normalizer = StandardScaler(withMean=True)\n",
    "cols = ['all_features']\n",
    "output_cols = 'features'\n",
    "fe_df = normalize_features(fe_df, cols, normalizer, output_cols, if_drop=False)\n",
    "\n",
    "## split train, test df\n",
    "fe_train = fe_df.filter(fe_df['is_test'] == 0)\n",
    "final_test = fe_df.filter(fe_df['is_test'] == 1).select(['sig_id', 'features'])\n",
    "\n",
    "## join training target with training features\n",
    "labels = target_df.drop('sig_id').columns\n",
    "final_train = fe_train.join(target_df, ['sig_id']).select(*(['sig_id','features'] + labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train test split\n",
    "(train, validation) = final_train.randomSplit([0.8, 0.2], 16)\n",
    "\n",
    "train.cache()\n",
    "validation.cache()\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "target_df.unpersist()\n",
    "fe_train.unpersist()\n",
    "fe_df.unpersist()\n",
    "final_train.unpersist()\n",
    "encoded_df.unpersist()\n",
    "corr_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Multilabel Classifier\n",
    "class MultiLabelClassifier:\n",
    "    \n",
    "    def __init__(self, clf, labels, feature_col,  \n",
    "                 hyperparameters={}, \n",
    "                 predict_col=['probability','prediction'],\n",
    "                 method=lambda prob_col, pred_col: float(pred_col if len(prob_col) == 1 else prob_col[1])):\n",
    "        '''\n",
    "        Initialize a multilabelclassifier\n",
    "        clf: the model to use\n",
    "        labels: a list of labels to predict\n",
    "        feature_col: the feature column\n",
    "        predict_col: the prediction column where the prediction is located\n",
    "        hyperparameters: all optional hyperparameters that can tune\n",
    "        method: a method of how to get the final prediction for one class\n",
    "        '''\n",
    "        self.clf = clf\n",
    "        self.labels = labels\n",
    "        self.feature_col = feature_col\n",
    "        self.predict_col = predict_col\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.method = method\n",
    "        self._trained_clfs = []\n",
    "\n",
    "    def fit(self, train):\n",
    "        train.cache()\n",
    "        self._trained_clfs = [self.clf(labelCol=label, featuresCol=self.feature_col, **self.hyperparameters)\n",
    "                              .fit(train) \n",
    "                              for label in tqdm(self.labels)]\n",
    "        train.unpersist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_test):\n",
    "        # convert method to udf\n",
    "        get_predict = udf(self.method,FloatType())\n",
    "        #target assembler\n",
    "        va = VectorAssembler(inputCols=self.labels, outputCol='targets')\n",
    "        ## transform this vector self.output_col to an array\n",
    "        select_cols = [self.feature_col, 'targets', 'sig_id']\n",
    "        res = va.transform(x_test).select(*select_cols)\n",
    "        for i, clf in tqdm(enumerate(self._trained_clfs)):\n",
    "            res = clf.transform(res)\n",
    "            new_col = self.labels[i]\n",
    "            res = res.withColumn(new_col, get_predict(*self.predict_col))\n",
    "            select_cols.append(new_col)\n",
    "            res = res.select(*select_cols)\n",
    "        self.res = res\n",
    "        return res.select(*select_cols[2:])\n",
    "    \n",
    "    def score(self):\n",
    "        #target assembler \n",
    "        va = VectorAssembler(inputCols=self.labels, outputCol='predicts')\n",
    "        ## transform this vector self.output_col to an array\n",
    "        df = va.transform(self.res).select('targets', 'predicts')\n",
    "        df = df.withColumn('targets', vector_to_array('targets'))\n",
    "        df = df.withColumn('predicts', vector_to_array('predicts'))\n",
    "        import math\n",
    "        @udf('double')\n",
    "        def log_loss(y, y_hat):\n",
    "            r = 0\n",
    "            cut = 1e-15\n",
    "            for t, p in zip(y, y_hat):\n",
    "                p = max(min(p, 1-cut),cut)\n",
    "                r += t * math.log(p) + (1 - t) * math.log(1 - p)\n",
    "            return r/len(y)\n",
    "        df = df.select(log_loss('targets','predicts').alias('log_loss'))\n",
    "        return df.select((-_mean(col('log_loss'))).alias('score'))\n",
    "    \n",
    "    def param_search_cv(self, train, grid_map, fold_num):\n",
    "                   \n",
    "        train.cache()\n",
    "        best_params = []\n",
    "        \n",
    "        def _extract_best_params(cv_model):\n",
    "        \n",
    "            best_param_dict = {}\n",
    "            scores = cv_model.avgMetrics\n",
    "            best_param = cv_model.getEstimatorParamMaps()[scores.index(min(scores))]\n",
    "\n",
    "            for k, v in best_param.items():\n",
    "                best_param_dict[k.name] = v\n",
    "\n",
    "            return best_param_dict\n",
    "            \n",
    "        for label in tqdm(self.labels):\n",
    "            \n",
    "            clf = self.clf(labelCol=label, featuresCol=self.feature_col)\n",
    "            evaluator = MultilabelEvaluator(method=self.method, predictionCol=self.predict_col, labelCol=label)\n",
    "            cv = CrossValidator(estimator=clf, estimatorParamMaps=grid_map, numFolds=fold_num, evaluator=evaluator, parallelism=3)\n",
    "            cv_model = cv.fit(train)\n",
    "            best_model = cv_model.bestModel\n",
    "            best_param = _extract_best_params(cv_model)\n",
    "\n",
    "            self._trained_clfs.append(best_model)\n",
    "            best_params.append(best_param) \n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "\n",
    "class MultilabelEvaluator(Evaluator):\n",
    "    \n",
    "    def __init__(self, method, predictionCol=['probability', 'prediction'], labelCol=\"label\"):\n",
    "        \n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.method = method\n",
    "        \n",
    "    def _evaluate(self, dataset):\n",
    "        \n",
    "        get_predict = udf(self.method,FloatType())\n",
    "        dataset = dataset.withColumn('pred_prob', get_predict(*self.predictionCol))\n",
    "        cut = 1e-15\n",
    "        new_dataset = dataset.select((-col(self.labelCol) * _log(col('pred_prob') + cut) - (1.0 - col(self.labelCol)) * _log(1.0 - col('pred_prob') + cut)).alias('log_loss'))        \n",
    "        score = new_dataset.select(_mean(col('log_loss')).alias('score')).collect()[0]['score']\n",
    "        return score\n",
    "        \n",
    "    def isLargerBetter(self):\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_array(prob_col, pred_col):\n",
    "            ## solve na problem, if len(prob_col) == 1, we use the prediction col\n",
    "            converted_prob_col = prob_col.toArray().tolist()\n",
    "            \n",
    "            if len(converted_prob_col) == 1:\n",
    "                \n",
    "                return pred_col\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                return converted_prob_col[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test\n",
    "clf = DecisionTreeClassifier\n",
    "grid_map = ParamGridBuilder().addGrid(clf().maxDepth, [2, 5]).addGrid(clf().impurity, ['gini', 'entropy']).build()\n",
    "method = convert_to_array\n",
    "dt = MultiLabelClassifier(clf, labels, 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = dt.param_search_cv(train, grid_map, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# print('Start calculating')\n",
    "# sample_id = \"sig_id\"\n",
    "# start = time.time()\n",
    "# y = target_df.select(*[sample_id] + labels)\n",
    "# score(y, res1, sample_id).show()\n",
    "# print('Calculation finished with time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_validation(df, estimator, k=2):\n",
    "    \n",
    "#     num_per_fold = 100 // k * 0.01\n",
    "#     num = [num_per_fold] * (k - 1) + [1 - num_per_fold * (k - 1)] \n",
    "#     k_fold_df = df.randomSplit(num)\n",
    "#     acc = []\n",
    "    \n",
    "#     def unionAll(*dfs):\n",
    "#         return reduce(DataFrame.unionAll, dfs)\n",
    "    \n",
    "#     for i, fold in enumerate(k_fold_df):\n",
    "        \n",
    "#         validation = fold\n",
    "#         train = unionAll(*[k_fold_df[j] for j in range(len(k_fold_df)) if i != j])\n",
    "        \n",
    "#         clf = estimator.fit(train)\n",
    "#         res = clf.transform(validation)\n",
    "#         score = clf.score().collect()[0]['score']\n",
    "        \n",
    "#         validation.unpersist()\n",
    "#         train.unpersist()\n",
    "#         estimator.reset()\n",
    "        \n",
    "#         acc.append(score)\n",
    "        \n",
    "#     mean_acc =  sum(acc) / k\n",
    "    \n",
    "#     print(f'the acc are {acc}, the mean acc is {mean_acc}')\n",
    "    \n",
    "#     return mean_acc\n",
    "\n",
    "# def extract_params(grid_map):\n",
    "    \n",
    "#     output_lst = []\n",
    "    \n",
    "#     for params in grid_map:\n",
    "#         temp_dict = {}\n",
    "        \n",
    "#         for k, v in params.items():\n",
    "#             temp_dict[k.name] = v\n",
    "        \n",
    "#         output_lst.append(temp_dict)\n",
    "        \n",
    "#     return output_lst\n",
    "\n",
    "# def grid_search(train, estimator, grid_map, k=2):\n",
    "    \n",
    "#     params = extract_params(grid_map)\n",
    "#     results = {}\n",
    "    \n",
    "#     for param in params:\n",
    "#         print(f'evaluating param {param}')\n",
    "        \n",
    "#         estimator.hyperparameters = param\n",
    "#         result = cross_validation(train, estimator, k)\n",
    "        \n",
    "#         if result in results.keys():\n",
    "#             results[result].append(param)\n",
    "            \n",
    "#         else:\n",
    "#             results[result] = [param]\n",
    "\n",
    "#     return param\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
